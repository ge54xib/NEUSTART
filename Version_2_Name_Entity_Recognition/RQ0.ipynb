{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc8beb62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up cleaning tools...\n",
      "Loading output.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-15 11:10:14,351 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 528 documents...\n",
      "Fitting Hyper-Granular Model (this may generate hundreds of topics)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af499a3d074646fdb1e6f1ac2d41033b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-15 11:10:25,844 - BERTopic - Embedding - Completed ✓\n",
      "2026-01-15 11:10:25,847 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-15 11:10:27,032 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-15 11:10:27,034 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-01-15 11:10:27,129 - BERTopic - Cluster - Completed ✓\n",
      "2026-01-15 11:10:27,139 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-15 11:10:27,269 - BERTopic - Representation - Completed ✓\n",
      "100%|██████████| 1/1 [00:00<00:00, 126.51it/s]\n",
      "2026-01-15 11:10:27,595 - BERTopic - WARNING: Using a custom list of topic assignments may lead to errors if topic reduction techniques are used afterwards. Make sure that manually assigning topics is the last step in the pipeline.Note that topic embeddings will also be created through weightedc-TF-IDF embeddings instead of centroid embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outliers reduced successfully.\n",
      "\n",
      "Total Topics Found: 107\n",
      "Saved to 'hyper_granular_topics.csv'\n",
      "Visualization saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from hdbscan import HDBSCAN\n",
    "from umap import UMAP\n",
    "\n",
    "# --- 1. PREPROCESSING SETUP ---\n",
    "print(\"Setting up cleaning tools...\")\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "custom_stops = {'page', 'http', 'https', 'www', 'com', 'paragraph', 'table', 'figure', 'section'} \n",
    "stop_words.update(custom_stops)\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    # Lowercase & remove non-text characters\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # Tokenize, remove stopwords, lemmatize\n",
    "    words = [lemmatizer.lemmatize(w) for w in text.split() if w not in stop_words and len(w) > 2]\n",
    "    return \" \".join(words)\n",
    "\n",
    "# --- 2. LOAD DATA ---\n",
    "csv_path = \"output.csv\"\n",
    "print(f\"Loading {csv_path}...\")\n",
    "df = pd.read_csv(csv_path)\n",
    "df = df.dropna(subset=['text'])\n",
    "\n",
    "# Apply cleaning\n",
    "df['clean_text'] = df['text'].apply(clean_text)\n",
    "df = df[df['clean_text'].str.len() > 5] # Remove empty results\n",
    "docs = df['clean_text'].tolist()\n",
    "\n",
    "print(f\"Processing {len(docs)} documents...\")\n",
    "\n",
    "# --- 3. CONFIGURE \"HYPER-GRANULAR\" SETTINGS ---\n",
    "\n",
    "# UMAP: Local Focus\n",
    "# n_neighbors=2 forces the model to only look at very immediate similarities.\n",
    "# This prevents it from grouping vaguely similar ideas together.\n",
    "umap_model = UMAP(\n",
    "    n_neighbors=2,       # EXTREME GRANULARITY (Default is 15)\n",
    "    n_components=5, \n",
    "    min_dist=0.0, \n",
    "    metric='cosine'\n",
    ")\n",
    "\n",
    "# HDBSCAN: Tiny Clusters\n",
    "# min_cluster_size=3 means a topic can be created with just 3 paragraphs.\n",
    "# min_samples=1 ensures almost NO data is treated as noise/outliers.\n",
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size=3,  # EXTREME GRANULARITY (Default is 10)\n",
    "    min_samples=1,       # PREVENTS OUTLIERS (Default is usually larger)\n",
    "    metric='euclidean', \n",
    "    cluster_selection_method='eom', \n",
    "    prediction_data=True\n",
    ")\n",
    "\n",
    "# Vectorizer: Allow rare words to define topics\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\", min_df=2)\n",
    "\n",
    "# --- 4. RUN MODEL ---\n",
    "print(\"Fitting Hyper-Granular Model (this may generate hundreds of topics)...\")\n",
    "\n",
    "topic_model = BERTopic(\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    language=\"english\",\n",
    "    verbose=True,\n",
    "    nr_topics=None # CRITICAL: Do not merge topics\n",
    ")\n",
    "\n",
    "topics, probs = topic_model.fit_transform(docs)\n",
    "\n",
    "# --- 5. REDUCE REMAINING OUTLIERS ---\n",
    "# With min_samples=1, outliers should be low, but this forces 100% assignment\n",
    "try:\n",
    "    new_topics = topic_model.reduce_outliers(docs, topics)\n",
    "    topic_model.update_topics(docs, topics=new_topics)\n",
    "    print(\"Outliers reduced successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"Skipping outlier reduction (likely almost 0 outliers exists).\")\n",
    "\n",
    "# --- 6. EXPORT RESULTS ---\n",
    "freq = topic_model.get_topic_info()\n",
    "print(f\"\\nTotal Topics Found: {len(freq) - 1}\")\n",
    "\n",
    "# Export to CSV\n",
    "freq.to_csv(\"hyper_granular_topics.csv\", index=False)\n",
    "print(\"Saved to 'hyper_granular_topics.csv'\")\n",
    "\n",
    "# Visualize top 50 (since you will have many)\n",
    "fig = topic_model.visualize_barchart(top_n_topics=50)\n",
    "fig.write_html(\"hyper_granular_chart.html\")\n",
    "print(\"Visualization saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2d0842c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Exported 14 topics to 'all_topics_list.csv'.\n",
      "\n",
      "--- ALL TOPICS ---\n",
      "    Topic  Count                                               Name\n",
      "0      -1    206             -1_quantum_technology_research_science\n",
      "1       0     56               0_canada_canadian_quantum_government\n",
      "2       1     49                  1_talent_skill_workforce_training\n",
      "3       2     39            2_quantum_technology_computing_computer\n",
      "4       3     28    3_postquantum_cryptography_communication_secure\n",
      "5       4     23                  4_quantum_regulatory_business_use\n",
      "6       5     23           5_computing_computer_quantum_application\n",
      "7       6     22  6_international_collaboration_partner_partnership\n",
      "8       7     20               7_quantum_bmwk_communication_imaging\n",
      "9       8     16                       8_government_set_qi_strategy\n",
      "10      9     13                   9_security_risk_national_control\n",
      "11     10     13                      10_program_million_year_grant\n",
      "12     11     10         11_direct_available_innovative_institution\n",
      "13     12     10               12_european_europe_ecosystem_partner\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Get All Topic Information ---\n",
    "\n",
    "# Get the full dataframe of topic info\n",
    "topic_info = topic_model.get_topic_info()\n",
    "\n",
    "# Method A: Save to CSV (Recommended for viewing all topics easily)\n",
    "topic_info.to_csv(\"all_topics_list.csv\", index=False)\n",
    "print(f\"Success! Exported {len(topic_info)} topics to 'all_topics_list.csv'.\")\n",
    "\n",
    "# Method B: Print all topics to the console\n",
    "# We change pandas settings to ensure rows aren't hidden\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "print(\"\\n--- ALL TOPICS ---\")\n",
    "print(topic_info[['Topic', 'Count', 'Name']])\n",
    "\n",
    "# Reset pandas options (optional)\n",
    "pd.reset_option('display.max_rows')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
