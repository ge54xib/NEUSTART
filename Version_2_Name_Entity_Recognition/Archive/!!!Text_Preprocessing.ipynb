{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73fb102b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Flair device: cpu\n",
      "Loading NER model...\n",
      "2026-01-19 15:25:53,716 SequenceTagger predicts: Dictionary with 20 tags: <unk>, O, S-ORG, S-MISC, B-PER, E-PER, S-LOC, B-ORG, E-ORG, I-PER, S-PER, B-MISC, I-MISC, E-MISC, I-ORG, B-LOC, E-LOC, I-LOC, <START>, <STOP>\n",
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import flair\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "# --- 1. Setup Environment ---\n",
    "\n",
    "# Download NLTK tokenizer if missing\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt_tab')\n",
    "except LookupError:\n",
    "    nltk.download('punkt_tab')\n",
    "\n",
    "# Set device (GPU is much faster if available)\n",
    "flair.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using Flair device: {flair.device}\")\n",
    "\n",
    "# Load the NER model once to avoid reloading it in loops\n",
    "# 'ner-fast' is recommended for speed; use 'ner' for slightly higher accuracy\n",
    "print(\"Loading NER model...\")\n",
    "tagger = SequenceTagger.load('ner-fast')\n",
    "print(\"Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83de2cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 files. Processing into paragraphs...\n",
      "Success! Paragraph data written to: output.csv\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Process Files & Split by PARAGRAPH ---\n",
    "\n",
    "def process_folder_to_paragraphs(input_folder_path, output_csv_path):\n",
    "    input_dir = Path(input_folder_path)\n",
    "    \n",
    "    if not input_dir.exists():\n",
    "        print(f\"Error: Directory '{input_folder_path}' not found.\")\n",
    "        return\n",
    "\n",
    "    with open(output_csv_path, 'w', newline='', encoding='utf-8') as out_f:\n",
    "        writer = csv.writer(out_f)\n",
    "        \n",
    "        # Header (Changed 'Sentence_ID' to 'Paragraph_ID')\n",
    "        header = [\"Paragraph_ID\", \"Doc_ID\", \"Country\", \"Year\", \"Document_Name\", \"text\"]\n",
    "        writer.writerow(header)\n",
    "\n",
    "        text_files = list(input_dir.glob(\"*.txt\"))\n",
    "        \n",
    "        print(f\"Found {len(text_files)} files. Processing into paragraphs...\")\n",
    "\n",
    "        for doc_id, file_path in enumerate(text_files, start=1):\n",
    "            try:\n",
    "                # Metadata Extraction\n",
    "                parts = file_path.stem.split(\"_\")\n",
    "                country = parts[0]\n",
    "                year = parts[1] if len(parts) > 1 and parts[1].isdigit() else \"Unknown\"\n",
    "                start_index = 2 if len(parts) > 1 and parts[1].isdigit() else 1\n",
    "                doc_name = \"_\".join(parts[start_index:]) if len(parts) > start_index else \"Unknown\"\n",
    "\n",
    "                # --- READ & SPLIT BY PARAGRAPH ---\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    raw_content = f.read()\n",
    "\n",
    "                # 1. Split by double newlines (standard paragraph separator)\n",
    "                raw_paragraphs = raw_content.split('\\n\\n')\n",
    "\n",
    "                para_id = 1\n",
    "                for p in raw_paragraphs:\n",
    "                    # 2. Clean up the paragraph: \n",
    "                    # Replace single newlines inside the paragraph with spaces\n",
    "                    # Strip leading/trailing whitespace\n",
    "                    clean_para = p.replace('\\n', ' ').strip()\n",
    "\n",
    "                    # Skip empty paragraphs\n",
    "                    if len(clean_para) < 5: \n",
    "                        continue\n",
    "\n",
    "                    writer.writerow([\n",
    "                        para_id,        # Paragraph ID\n",
    "                        doc_id,\n",
    "                        country,\n",
    "                        year,\n",
    "                        doc_name,\n",
    "                        clean_para      # The full paragraph text\n",
    "                    ])\n",
    "                    para_id += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_path.name}: {e}\")\n",
    "\n",
    "    print(f\"Success! Paragraph data written to: {output_csv_path}\")\n",
    "\n",
    "# Run the function\n",
    "input_folder = \"countries_edited\" \n",
    "intermediate_file = \"output.csv\"\n",
    "\n",
    "process_folder_to_paragraphs(input_folder, intermediate_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c14787a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 files. Processing and splitting sentences...\n",
      "Success! Pre-processed data written to: sentences_master.csv\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Process Files & Split Sentences (NLTK) ---\n",
    "\n",
    "def process_folder_to_csv(input_folder_path, output_csv_path):\n",
    "    input_dir = Path(input_folder_path)\n",
    "    \n",
    "    # Check input directory\n",
    "    if not input_dir.exists():\n",
    "        print(f\"Error: Directory '{input_folder_path}' not found.\")\n",
    "        return\n",
    "\n",
    "    # Prepare to write\n",
    "    with open(output_csv_path, 'w', newline='', encoding='utf-8') as out_f:\n",
    "        writer = csv.writer(out_f)\n",
    "        \n",
    "        # Write Header\n",
    "        header = [\"Sentence_ID\", \"Doc_ID\", \"Country\", \"Year\", \"Document_Name\", \"text\"]\n",
    "        writer.writerow(header)\n",
    "\n",
    "        text_files = list(input_dir.glob(\"*.txt\"))\n",
    "        if not text_files:\n",
    "            print(\"No .txt files found.\")\n",
    "            return\n",
    "\n",
    "        print(f\"Found {len(text_files)} files. Processing and splitting sentences...\")\n",
    "\n",
    "        for doc_id, file_path in enumerate(text_files, start=1):\n",
    "            try:\n",
    "                # Parse filename: Country_Year_DocumentName.txt\n",
    "                parts = file_path.stem.split(\"_\")\n",
    "                country = parts[0]\n",
    "                year = parts[1] if len(parts) > 1 and parts[1].isdigit() else \"Unknown\"\n",
    "                \n",
    "                # Determine where the document name starts\n",
    "                start_index = 2 if len(parts) > 1 and parts[1].isdigit() else 1\n",
    "                doc_name = \"_\".join(parts[start_index:]) if len(parts) > start_index else \"Unknown\"\n",
    "\n",
    "                # Read text\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    # Replace newlines with spaces to keep sentences intact across line breaks\n",
    "                    text = f.read().replace('\\n', ' ')\n",
    "\n",
    "                if not text.strip():\n",
    "                    continue\n",
    "\n",
    "                # --- SPLITTING HAPPENS HERE (ONCE ONLY) ---\n",
    "                sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "                for sent_idx, sentence in enumerate(sentences, start=1):\n",
    "                    writer.writerow([\n",
    "                        sent_idx,       # Resets per file\n",
    "                        doc_id,         # Unique ID per file\n",
    "                        country,\n",
    "                        year,\n",
    "                        doc_name,\n",
    "                        sentence.strip()\n",
    "                    ])\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_path.name}: {e}\")\n",
    "\n",
    "    print(f\"Success! Pre-processed data written to: {output_csv_path}\")\n",
    "\n",
    "# Run the function\n",
    "# Ensure 'countries_edited' folder exists\n",
    "input_folder = \"countries_edited\" \n",
    "intermediate_file = \"sentences_master.csv\"\n",
    "\n",
    "process_folder_to_csv(input_folder, intermediate_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51478e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading paragraph CSV...\n",
      "Running NER on 1986 paragraphs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch inference:   2%|‚ñè         | 1/63 [00:17<18:09, 17.57s/it]"
     ]
    }
   ],
   "source": [
    "# --- 3. Run NER on Paragraphs & Deduplicate ---\n",
    "\n",
    "print(\"Reading paragraph CSV...\")\n",
    "df = pd.read_csv(intermediate_file)\n",
    "\n",
    "# Ensure text column is string\n",
    "df[\"text\"] = df[\"text\"].fillna(\"\").astype(str)\n",
    "\n",
    "# Prepare Flair Sentences (Each \"Sentence\" object is now actually a full Paragraph)\n",
    "flair_paragraphs = []\n",
    "valid_indices = [] \n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    text = row['text']\n",
    "    # Skip short/garbage text\n",
    "    if len(text) < 5: continue\n",
    "        \n",
    "    # Create Flair Sentence (Here it holds a whole paragraph)\n",
    "    # Flair can handle this, but ensures the model sees the local context\n",
    "    sent_obj = Sentence(text)\n",
    "    flair_paragraphs.append(sent_obj)\n",
    "    valid_indices.append(idx)\n",
    "\n",
    "print(f\"Running NER on {len(flair_paragraphs)} paragraphs...\")\n",
    "\n",
    "# Batch Prediction\n",
    "BATCH_SIZE = 256 if torch.cuda.is_available() else 32\n",
    "tagger.predict(flair_paragraphs, mini_batch_size=BATCH_SIZE, verbose=True)\n",
    "\n",
    "# --- Deduplication (Per Document) ---\n",
    "print(\"Extracting entities...\")\n",
    "\n",
    "final_rows = []\n",
    "seen_entities_per_doc = set()\n",
    "\n",
    "for i, sent_obj in enumerate(flair_paragraphs):\n",
    "    original_idx = valid_indices[i]\n",
    "    original_row = df.iloc[original_idx]\n",
    "    current_doc_id = original_row[\"Doc_ID\"]\n",
    "\n",
    "    for entity in sent_obj.get_spans('ner'):\n",
    "        if entity.tag in ['ORG', 'PER']:\n",
    "            \n",
    "            dedup_key = (current_doc_id, entity.text, entity.tag)\n",
    "            \n",
    "            if dedup_key not in seen_entities_per_doc:\n",
    "                seen_entities_per_doc.add(dedup_key)\n",
    "                \n",
    "                final_rows.append({\n",
    "                    \"Paragraph_ID\": original_row[\"Paragraph_ID\"],\n",
    "                    \"Doc_ID\": current_doc_id,\n",
    "                    \"Country\": original_row[\"Country\"],\n",
    "                    \"Year\": original_row[\"Year\"],\n",
    "                    \"Document_Name\": original_row[\"Document_Name\"],\n",
    "                    \"entity_name\": entity.text,\n",
    "                    \"ner_label\": entity.tag,\n",
    "                    \"qh_category\": \"\", \n",
    "                    \"qh_sub_category\": \"\",\n",
    "                    \"qh_exact_category\": \"\"\n",
    "                })\n",
    "\n",
    "output_df = pd.DataFrame(final_rows)\n",
    "output_filename = \"entities.csv\"\n",
    "output_df.to_csv(output_filename, index=False)\n",
    "output_filename = \"entities_to_edit.csv\"\n",
    "output_df.to_csv(output_filename, index=False)\n",
    "\n",
    "print(f\"--- Processing Complete ---\")\n",
    "print(f\"Saved to: {output_filename}\")\n",
    "output_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57875ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading pre-processed CSV...\n",
      "Running NER on 1993 sentences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch inference:   0%|          | 0/63 [00:09<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Batch Prediction\u001b[39;00m\n\u001b[32m     24\u001b[39m BATCH_SIZE = \u001b[32m256\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m32\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[43mtagger\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflair_sentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmini_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# --- Deduplication Logic ---\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mExtracting entities and removing duplicates within documents...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\OneDrive - TUM\\Master Thesis\\Analysis\\NEUSTART\\.venv\\Lib\\site-packages\\flair\\models\\sequence_tagger_model.py:514\u001b[39m, in \u001b[36mSequenceTagger.predict\u001b[39m\u001b[34m(self, sentences, mini_batch_size, return_probabilities_for_all_classes, verbose, label_name, return_loss, embedding_storage_mode, force_token_predictions)\u001b[39m\n\u001b[32m    511\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    513\u001b[39m \u001b[38;5;66;03m# get features from forward propagation\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m514\u001b[39m sentence_tensor, lengths = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    515\u001b[39m features = \u001b[38;5;28mself\u001b[39m.forward(sentence_tensor, lengths)\n\u001b[32m    517\u001b[39m \u001b[38;5;66;03m# remove previously predicted labels of this type\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\OneDrive - TUM\\Master Thesis\\Analysis\\NEUSTART\\.venv\\Lib\\site-packages\\flair\\models\\sequence_tagger_model.py:304\u001b[39m, in \u001b[36mSequenceTagger._prepare_tensors\u001b[39m\u001b[34m(self, data_points)\u001b[39m\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_prepare_tensors\u001b[39m(\u001b[38;5;28mself\u001b[39m, data_points: Union[\u001b[38;5;28mlist\u001b[39m[Sentence], Sentence]) -> \u001b[38;5;28mtuple\u001b[39m[torch.Tensor, torch.LongTensor]:\n\u001b[32m    303\u001b[39m     sentences = [data_points] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_points, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m data_points\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    306\u001b[39m     \u001b[38;5;66;03m# make a zero-padded tensor for the whole sentence\u001b[39;00m\n\u001b[32m    307\u001b[39m     lengths, sentence_tensor = \u001b[38;5;28mself\u001b[39m._make_padded_tensor_for_batch(sentences)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\OneDrive - TUM\\Master Thesis\\Analysis\\NEUSTART\\.venv\\Lib\\site-packages\\flair\\embeddings\\token.py:97\u001b[39m, in \u001b[36mStackedEmbeddings.embed\u001b[39m\u001b[34m(self, sentences, static_embeddings)\u001b[39m\n\u001b[32m     94\u001b[39m     sentences = [sentences]\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m embedding \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.embeddings:\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m     \u001b[43membedding\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\OneDrive - TUM\\Master Thesis\\Analysis\\NEUSTART\\.venv\\Lib\\site-packages\\flair\\embeddings\\base.py:51\u001b[39m, in \u001b[36mEmbeddings.embed\u001b[39m\u001b[34m(self, data_points)\u001b[39m\n\u001b[32m     48\u001b[39m     data_points = [data_points]\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._everything_embedded(data_points):\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_add_embeddings_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_points\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m data_points\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\OneDrive - TUM\\Master Thesis\\Analysis\\NEUSTART\\.venv\\Lib\\site-packages\\flair\\embeddings\\token.py:808\u001b[39m, in \u001b[36mFlairEmbeddings._add_embeddings_internal\u001b[39m\u001b[34m(self, sentences)\u001b[39m\n\u001b[32m    805\u001b[39m end_marker = \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    807\u001b[39m \u001b[38;5;66;03m# get hidden states from language model\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m808\u001b[39m all_hidden_states_in_lm = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_representation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    809\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext_sentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_marker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_marker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchars_per_chunk\u001b[49m\n\u001b[32m    810\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    812\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fine_tune:\n\u001b[32m    813\u001b[39m     all_hidden_states_in_lm = all_hidden_states_in_lm.detach()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\OneDrive - TUM\\Master Thesis\\Analysis\\NEUSTART\\.venv\\Lib\\site-packages\\flair\\models\\language_model.py:160\u001b[39m, in \u001b[36mLanguageModel.get_representation\u001b[39m\u001b[34m(self, strings, start_marker, end_marker, chars_per_chunk)\u001b[39m\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m batches:\n\u001b[32m    159\u001b[39m     batch = batch.transpose(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     rnn_output, hidden = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m     output_parts.append(rnn_output)\n\u001b[32m    163\u001b[39m \u001b[38;5;66;03m# concatenate all chunks to make final output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\OneDrive - TUM\\Master Thesis\\Analysis\\NEUSTART\\.venv\\Lib\\site-packages\\flair\\models\\language_model.py:88\u001b[39m, in \u001b[36mLanguageModel.forward\u001b[39m\u001b[34m(self, input, hidden, ordered_sequence_lengths, decode)\u001b[39m\n\u001b[32m     86\u001b[39m     hidden = (h,)\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m     output, hidden = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43memb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.proj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     91\u001b[39m     output = \u001b[38;5;28mself\u001b[39m.proj(output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\OneDrive - TUM\\Master Thesis\\Analysis\\NEUSTART\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\OneDrive - TUM\\Master Thesis\\Analysis\\NEUSTART\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\OneDrive - TUM\\Master Thesis\\Analysis\\NEUSTART\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:1127\u001b[39m, in \u001b[36mLSTM.forward\u001b[39m\u001b[34m(self, input, hx)\u001b[39m\n\u001b[32m   1124\u001b[39m         hx = \u001b[38;5;28mself\u001b[39m.permute_hidden(hx, sorted_indices)\n\u001b[32m   1126\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1127\u001b[39m     result = \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1128\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1129\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1130\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1131\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1132\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1133\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1134\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1135\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1136\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1137\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1138\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1139\u001b[39m     result = _VF.lstm(\n\u001b[32m   1140\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   1141\u001b[39m         batch_sizes,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1148\u001b[39m         \u001b[38;5;28mself\u001b[39m.bidirectional,\n\u001b[32m   1149\u001b[39m     )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# --- 3. Run NER on sentences & Deduplicate per Document ---\n",
    "\n",
    "print(\"Reading pre-processed CSV...\")\n",
    "df = pd.read_csv(intermediate_file)\n",
    "\n",
    "# Ensure text column is string\n",
    "df[\"text\"] = df[\"text\"].fillna(\"\").astype(str)\n",
    "\n",
    "# Prepare Flair Sentences\n",
    "flair_sentences = []\n",
    "valid_indices = [] \n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    text = row['text']\n",
    "    if len(text) < 2: continue\n",
    "        \n",
    "    sent_obj = Sentence(text)\n",
    "    flair_sentences.append(sent_obj)\n",
    "    valid_indices.append(idx)\n",
    "\n",
    "print(f\"Running NER on {len(flair_sentences)} sentences...\")\n",
    "\n",
    "# Batch Prediction\n",
    "BATCH_SIZE = 256 if torch.cuda.is_available() else 32\n",
    "tagger.predict(flair_sentences, mini_batch_size=BATCH_SIZE, verbose=True)\n",
    "\n",
    "# --- Deduplication Logic ---\n",
    "\n",
    "print(\"Extracting entities and removing duplicates within documents...\")\n",
    "\n",
    "final_rows = []\n",
    "\n",
    "# This set tracks: (Doc_ID, Entity_Name, Label)\n",
    "# It ensures an entity is listed only ONCE per document, regardless of how many sentences it appears in.\n",
    "seen_entities_per_doc = set()\n",
    "\n",
    "for i, sent_obj in enumerate(flair_sentences):\n",
    "    # Retrieve metadata for this sentence\n",
    "    original_idx = valid_indices[i]\n",
    "    original_row = df.iloc[original_idx]\n",
    "    \n",
    "    current_doc_id = original_row[\"Doc_ID\"] # The scope for our deduplication\n",
    "\n",
    "    for entity in sent_obj.get_spans('ner'):\n",
    "        if entity.tag in ['ORG', 'PER']:\n",
    "            \n",
    "            # Create a unique key for this specific document\n",
    "            dedup_key = (current_doc_id, entity.text, entity.tag)\n",
    "            \n",
    "            # Only add if we haven't seen this entity in this specific document yet\n",
    "            if dedup_key not in seen_entities_per_doc:\n",
    "                seen_entities_per_doc.add(dedup_key)\n",
    "                \n",
    "                final_rows.append({\n",
    "                    \"Sentence_ID\": original_row[\"Sentence_ID\"], # Records the first sentence where it was found\n",
    "                    \"Doc_ID\": current_doc_id,\n",
    "                    \"Country\": original_row[\"Country\"],\n",
    "                    \"Year\": original_row[\"Year\"],\n",
    "                    \"Document_Name\": original_row[\"Document_Name\"],\n",
    "                    \"entity_name\": entity.text,\n",
    "                    \"ner_label\": entity.tag,\n",
    "                    \"qh_category\": \"\", \n",
    "                    \"qh_sub_category\": \"\",\n",
    "                    \"qh_exact_category\": \"\"\n",
    "                })\n",
    "\n",
    "output_df = pd.DataFrame(final_rows)\n",
    "output_filename = \"2entities.csv\"\n",
    "output_df.to_csv(output_filename, index=False)\n",
    "output_filename = \"2entities_to_edit.csv\"\n",
    "output_df.to_csv(output_filename, index=False)\n",
    "\n",
    "print(f\"--- Processing Complete ---\")\n",
    "print(f\"Extracted {len(output_df)} unique entities (deduplicated by Document).\")\n",
    "print(f\"Saved to: {output_filename}\")\n",
    "output_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ffc642f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'pandas' has no attribute 'core' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmerge_manual_edits\u001b[39m(fresh_data_path, manual_edits_path, output_path):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\OneDrive - TUM\\Master Thesis\\Analysis\\NEUSTART\\.venv\\Lib\\site-packages\\pandas\\__init__.py:151\u001b[39m\n\u001b[32m    133\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreshape\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    134\u001b[39m     concat,\n\u001b[32m    135\u001b[39m     lreshape,\n\u001b[32m   (...)\u001b[39m\u001b[32m    147\u001b[39m     qcut,\n\u001b[32m    148\u001b[39m )\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m api, arrays, errors, io, plotting, tseries\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m testing\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_print_versions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m show_versions\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    155\u001b[39m     \u001b[38;5;66;03m# excel\u001b[39;00m\n\u001b[32m    156\u001b[39m     ExcelFile,\n\u001b[32m   (...)\u001b[39m\u001b[32m    184\u001b[39m     read_spss,\n\u001b[32m    185\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\OneDrive - TUM\\Master Thesis\\Analysis\\NEUSTART\\.venv\\Lib\\site-packages\\pandas\\testing.py:6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03mPublic testing utility functions.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_testing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      7\u001b[39m     assert_extension_array_equal,\n\u001b[32m      8\u001b[39m     assert_frame_equal,\n\u001b[32m      9\u001b[39m     assert_index_equal,\n\u001b[32m     10\u001b[39m     assert_series_equal,\n\u001b[32m     11\u001b[39m )\n\u001b[32m     13\u001b[39m __all__ = [\n\u001b[32m     14\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33massert_extension_array_equal\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     15\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33massert_frame_equal\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     16\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33massert_series_equal\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     17\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33massert_index_equal\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     18\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\OneDrive - TUM\\Master Thesis\\Analysis\\NEUSTART\\.venv\\Lib\\site-packages\\pandas\\_testing\\__init__.py:405\u001b[39m\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpytest\u001b[39;00m\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pytest.raises(expected_exception, match=\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m405\u001b[39m cython_table = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcore\u001b[49m.common._cython_table.items()\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_cython_table_params\u001b[39m(ndframe, func_names_and_expected):\n\u001b[32m    409\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    410\u001b[39m \u001b[33;03m    Combine frame, functions from com._cython_table\u001b[39;00m\n\u001b[32m    411\u001b[39m \u001b[33;03m    keys and expected result.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    423\u001b[39m \u001b[33;03m        List of three items (DataFrame, function, expected result)\u001b[39;00m\n\u001b[32m    424\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: partially initialized module 'pandas' has no attribute 'core' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def merge_manual_edits(fresh_data_path, manual_edits_path, output_path):\n",
    "    print(f\"Loading '{fresh_data_path}' and '{manual_edits_path}'...\")\n",
    "    \n",
    "    # 1. Load DataFrames\n",
    "    df_fresh = pd.read_csv(fresh_data_path)       # The file you just generated (2entities.csv)\n",
    "    df_manual = pd.read_csv(manual_edits_path)    # The file with your manual work (entities_to_edit.csv)\n",
    "\n",
    "    # 2. Prepare for Merge\n",
    "    # We match on these 4 columns to ensure we don't mix up entities across documents\n",
    "    join_keys = ['Country', 'Year', 'Document_Name', 'entity_name']\n",
    "\n",
    "    # These are the columns we want to bring OVER from the manual file\n",
    "    cols_to_import = [\n",
    "        'qh_category', \n",
    "        'qh_sub_category', \n",
    "        'qh_exact_category', \n",
    "        'modified_entity_name', \n",
    "        'finalized_entity_name'\n",
    "    ]\n",
    "\n",
    "    # Clean up manual file before merging:\n",
    "    # Remove duplicates in manual file so we don't explode the fresh file\n",
    "    # (e.g., if \"Canada\" appears twice in manual file for the same doc, keep the first one)\n",
    "    df_manual_clean = df_manual.drop_duplicates(subset=join_keys)[join_keys + cols_to_import]\n",
    "\n",
    "    print(f\"Merging data...\")\n",
    "\n",
    "    # 3. Perform the Merge (Left Join)\n",
    "    # Left join = Keep everything in Fresh, attach Manual info where it exists\n",
    "    merged_df = df_fresh.merge(df_manual_clean, on=join_keys, how='left', suffixes=('_old', ''))\n",
    "\n",
    "    # 4. Cleanup Columns\n",
    "    # If the fresh file already had these columns (empty), the merge might create duplicates or NaNs.\n",
    "    # We ensure the final columns have the correct data.\n",
    "    \n",
    "    # If 'qh_category_old' exists (from the fresh file), we can drop it \n",
    "    # because we prefer the new column coming from the manual file.\n",
    "    for col in cols_to_import:\n",
    "        old_col = col + '_old'\n",
    "        if old_col in merged_df.columns:\n",
    "            merged_df.drop(columns=[old_col], inplace=True)\n",
    "\n",
    "    # Fill NaNs with empty strings for a cleaner CSV\n",
    "    merged_df[cols_to_import] = merged_df[cols_to_import].fillna(\"\")\n",
    "    \n",
    "    # 5. Save\n",
    "    merged_df.to_csv(output_path, index=False)\n",
    "    print(f\"Success! Merged file saved to: {output_path}\")\n",
    "    \n",
    "    # Show a quick preview of rows that actually got updated\n",
    "    updated_rows = merged_df[merged_df['qh_category'] != \"\"]\n",
    "    print(f\"Entities updated with manual info: {len(updated_rows)}\")\n",
    "    if not updated_rows.empty:\n",
    "        print(\"Preview of updated rows:\")\n",
    "        print(updated_rows[['entity_name', 'qh_category', 'finalized_entity_name']].head())\n",
    "    else:\n",
    "        print(\"Warning: No matching entities found. Check if entity names spellings match exactly.\")\n",
    "\n",
    "# --- Run the function ---\n",
    "# Input 1: The fresh file you just created in the previous step\n",
    "# Input 2: Your old file with manual edits\n",
    "# Output: The final combined file\n",
    "merge_manual_edits(\"2entities_to_edit.csv\", \"entities_to_edit.csv\", \"2entities_to_edit.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf729070",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'pandas' has no attribute 'core' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mipywidgets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwidgets\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIPython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m display, clear_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\OneDrive - TUM\\Master Thesis\\Analysis\\NEUSTART\\.venv\\Lib\\site-packages\\pandas\\__init__.py:151\u001b[39m\n\u001b[32m    133\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreshape\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    134\u001b[39m     concat,\n\u001b[32m    135\u001b[39m     lreshape,\n\u001b[32m   (...)\u001b[39m\u001b[32m    147\u001b[39m     qcut,\n\u001b[32m    148\u001b[39m )\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m api, arrays, errors, io, plotting, tseries\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m testing\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_print_versions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m show_versions\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    155\u001b[39m     \u001b[38;5;66;03m# excel\u001b[39;00m\n\u001b[32m    156\u001b[39m     ExcelFile,\n\u001b[32m   (...)\u001b[39m\u001b[32m    184\u001b[39m     read_spss,\n\u001b[32m    185\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\OneDrive - TUM\\Master Thesis\\Analysis\\NEUSTART\\.venv\\Lib\\site-packages\\pandas\\testing.py:6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03mPublic testing utility functions.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_testing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      7\u001b[39m     assert_extension_array_equal,\n\u001b[32m      8\u001b[39m     assert_frame_equal,\n\u001b[32m      9\u001b[39m     assert_index_equal,\n\u001b[32m     10\u001b[39m     assert_series_equal,\n\u001b[32m     11\u001b[39m )\n\u001b[32m     13\u001b[39m __all__ = [\n\u001b[32m     14\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33massert_extension_array_equal\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     15\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33massert_frame_equal\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     16\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33massert_series_equal\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     17\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33massert_index_equal\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     18\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\OneDrive - TUM\\Master Thesis\\Analysis\\NEUSTART\\.venv\\Lib\\site-packages\\pandas\\_testing\\__init__.py:405\u001b[39m\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpytest\u001b[39;00m\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pytest.raises(expected_exception, match=\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m405\u001b[39m cython_table = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcore\u001b[49m.common._cython_table.items()\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_cython_table_params\u001b[39m(ndframe, func_names_and_expected):\n\u001b[32m    409\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    410\u001b[39m \u001b[33;03m    Combine frame, functions from com._cython_table\u001b[39;00m\n\u001b[32m    411\u001b[39m \u001b[33;03m    keys and expected result.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    423\u001b[39m \u001b[33;03m        List of three items (DataFrame, function, expected result)\u001b[39;00m\n\u001b[32m    424\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: partially initialized module 'pandas' has no attribute 'core' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# --- 1. CONFIGURATION & DATA LOADING ---\n",
    "filename = \"2entities_to_edit.csv\"\n",
    "\n",
    "# --- TAXONOMY (General English) ---\n",
    "TAXONOMY = {\n",
    "    \"Academia\": {\n",
    "        \"Higher Education Institutions (HEIs)\": {\n",
    "            \"desc\": \"Universities, Colleges, Schools (Teaching & Research).\",\n",
    "            \"keywords\": [\n",
    "                \"University\", \"College\", \"School\", \"Academy\", \"Faculty\", \"Department\", \n",
    "                \"Chair\", \"Campus\", \"Institute of Technology\", \"Polytechnic\", \n",
    "                \"Business School\", \"Medical School\", \"Law School\"\n",
    "            ]\n",
    "        },\n",
    "        \"Public Research Orgs (PROs)\": {\n",
    "            \"desc\": \"Research Institutes (Knowledge Output, no teaching).\",\n",
    "            \"keywords\": [\n",
    "                \"Institute\", \"Center\", \"Centre\", \"Laboratory\", \"Lab\", \"Observatory\", \n",
    "                \"National Lab\", \"Research Council\", \"Think Tank\", \"Agency (Research)\"\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    \"Industry\": {\n",
    "        \"Private Firms (Corporates)\": {\n",
    "            \"desc\": \"Established Companies, SMEs, MNEs.\",\n",
    "            \"keywords\": [\n",
    "                \"Inc\", \"Corp\", \"Corporation\", \"Ltd\", \"LLC\", \"PLC\", \"Co\", \"Company\", \n",
    "                \"Group\", \"Holdings\", \"Manufacturer\", \"Supplier\", \"Vendor\", \n",
    "                \"Conglomerate\", \"Multinational\", \"Enterprise\", \"Firm\"\n",
    "            ]\n",
    "        },\n",
    "        \"Start-ups\": {\n",
    "            \"desc\": \"Young Growth Companies, Spin-offs.\",\n",
    "            \"keywords\": [\n",
    "                \"Start-up\", \"Startup\", \"Spin-off\", \"Spinoff\", \"Scale-up\", \"Unicorn\", \n",
    "                \"Venture\", \"NewCo\", \"DeepTech\", \"Founder\", \"Stealth Mode\"\n",
    "            ]\n",
    "        },\n",
    "        \"Consulting\": {\n",
    "            \"desc\": \"Services, Advisory, Legal, HR.\",\n",
    "            \"keywords\": [\n",
    "                \"Consulting\", \"Consultancy\", \"Advisors\", \"Partners\", \"Legal\", \"Law Firm\", \n",
    "                \"LLP\", \"Attorney\", \"IP Law\", \"Patent\", \"Audit\", \"Tax\", \"Recruitment\", \n",
    "                \"Headhunter\", \"Strategy\", \"Management\", \"Services\"\n",
    "            ]\n",
    "        },\n",
    "        \"Venture Capital / Investors\": {\n",
    "            \"desc\": \"Financial Actors, VCs, Business Angels.\",\n",
    "            \"keywords\": [\n",
    "                \"Capital\", \"Invest\", \"Investment\", \"Fund\", \"Venture\", \"VC\", \"Equity\", \n",
    "                \"Private Equity\", \"PE\", \"Angel\", \"Seed\", \"Asset Management\", \"Bank\", \n",
    "                \"Financial Group\", \"Holding\", \"Wealth Management\"\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    \"Government\": {\n",
    "        \"Policy Makers\": {\n",
    "            \"desc\": \"Ministries, Councils, Parliaments (Regulation).\",\n",
    "            \"keywords\": [\n",
    "                \"Ministry\", \"Department\", \"Dept\", \"Council\", \"Government\", \"Federal\", \n",
    "                \"State\", \"Municipality\", \"City\", \"County\", \"District\", \"Parliament\", \n",
    "                \"Senate\", \"Commission\", \"Mayor\", \"Governor\", \"Regulator\", \"Authority\", \n",
    "                \"Administration\", \"Bureau\"\n",
    "            ]\n",
    "        },\n",
    "        \"Funding Agencies\": {\n",
    "            \"desc\": \"Funding Bodies, Project Management Agencies.\",\n",
    "            \"keywords\": [\n",
    "                \"Foundation\", \"Agency\", \"Grant\", \"Funding\", \"Fund\", \"Endowment\", \n",
    "                \"Trust\", \"Award\", \"Scholarship\", \"Fellowship\", \"Program\", \"Initiative\"\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    \"Civil Society\": {\n",
    "        \"Media\": {\n",
    "            \"desc\": \"Press, News, Journals.\",\n",
    "            \"keywords\": [\n",
    "                \"News\", \"Journal\", \"Press\", \"Times\", \"Post\", \"Daily\", \"Review\", \n",
    "                \"Magazine\", \"Publisher\", \"Broadcaster\", \"TV\", \"Radio\", \"Podcast\", \n",
    "                \"Blog\", \"Media\", \"Outlet\", \"Chronicle\", \"Gazette\"\n",
    "            ]\n",
    "        },\n",
    "        \"Cultural Institutions\": {\n",
    "            \"desc\": \"Museums, Libraries, Galleries.\",\n",
    "            \"keywords\": [\n",
    "                \"Museum\", \"Library\", \"Gallery\", \"Theater\", \"Opera\", \"Orchestra\", \n",
    "                \"Archive\", \"Collection\", \"Exhibition\", \"Zoo\", \"Botanical Garden\", \n",
    "                \"Science Center\", \"Planetarium\", \"Hall\"\n",
    "            ]\n",
    "        },\n",
    "        \"NGOs / NPOs\": {\n",
    "            \"desc\": \"Non-Profit, Social Goals, Charities.\",\n",
    "            \"keywords\": [\n",
    "                \"Charity\", \"Non-Profit\", \"NPO\", \"NGO\", \"Organization\", \"Society\", \n",
    "                \"Club\", \"Union\", \"Alliance\", \"Federation\", \"Initiative\", \"Philanthropy\", \n",
    "                \"Foundation (Private)\", \"Mission\", \"Relief\"\n",
    "            ]\n",
    "        },\n",
    "        \"Intermediaries\": {\n",
    "            \"desc\": \"Clusters, Hubs, TTOs, Chambers.\",\n",
    "            \"keywords\": [\n",
    "                \"Cluster\", \"Network\", \"Hub\", \"Incubator\", \"Accelerator\", \"TTO\", \n",
    "                \"Technology Transfer\", \"Chamber of Commerce\", \"Trade Union\", \n",
    "                \"Association\", \"Consortium\", \"Standardization\", \"Body\", \"Council (Trade)\"\n",
    "            ]\n",
    "        },\n",
    "        \"Citizens / Users\": {\n",
    "            \"desc\": \"Citizens, Patients, User Groups.\",\n",
    "            \"keywords\": [\n",
    "                \"Community\", \"Group\", \"Public\", \"Citizen\", \"Patient\", \"User\", \n",
    "                \"Resident\", \"Population\", \"Crowd\", \"Forum\", \"Volunteer\", \"Advocacy\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(filename).fillna(\"\")\n",
    "    required_cols = [\"qh_category\", \"qh_sub_category\", \"qh_exact_category\", \"modified_entity_name\", \"finalized_entity_name\"]\n",
    "    for col in required_cols:\n",
    "        if col not in df.columns:\n",
    "            df[col] = \"\"\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: '{filename}' not found.\")\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "# --- 2. WIDGET SETUP ---\n",
    "\n",
    "if not df.empty:\n",
    "    unfinished_indices = df[df['qh_category'] == \"\"].index.tolist()\n",
    "    current_idx = unfinished_indices[0] if unfinished_indices else 0\n",
    "    NEW_OPT = \"+++ Create New +++\"\n",
    "\n",
    "    # Progress Bar\n",
    "    total_items = len(df)\n",
    "    progress = widgets.IntProgress(\n",
    "        value=len(df) - len(unfinished_indices),\n",
    "        min=0,\n",
    "        max=total_items,\n",
    "        description='Progress:',\n",
    "        bar_style='success',\n",
    "        layout=widgets.Layout(width='99%')\n",
    "    )\n",
    "    progress_label = widgets.Label(value=f\"{progress.value} / {total_items} tagged\")\n",
    "\n",
    "    # --- INPUT WIDGETS ---\n",
    "\n",
    "    # 0. Entity Name Editor\n",
    "    w_name_edit = widgets.Text(\n",
    "        description='<b>Edit Name:</b>',\n",
    "        placeholder='Correct the entity name here...',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout=widgets.Layout(width='600px')\n",
    "    )\n",
    "\n",
    "    # 1. Main Category\n",
    "    w_helix = widgets.Dropdown(\n",
    "        options=[''] + list(TAXONOMY.keys()) + [NEW_OPT],\n",
    "        description='1. Helix:',\n",
    "        layout=widgets.Layout(width='400px')\n",
    "    )\n",
    "    w_helix_new = widgets.Text(\n",
    "        placeholder='Type new Helix category...',\n",
    "        layout=widgets.Layout(width='400px', display='none') \n",
    "    )\n",
    "\n",
    "    # 2. Sub Category\n",
    "    w_sub = widgets.Dropdown(\n",
    "        options=[],\n",
    "        description='2. Type:',\n",
    "        layout=widgets.Layout(width='400px'),\n",
    "        disabled=True\n",
    "    )\n",
    "    w_sub_new = widgets.Text(\n",
    "        placeholder='Type new Sub-Category...',\n",
    "        layout=widgets.Layout(width='400px', display='none')\n",
    "    )\n",
    "\n",
    "    # 3. Exact Category (MULTI SELECT)\n",
    "    w_exact_multi = widgets.SelectMultiple(\n",
    "        options=[],\n",
    "        description='3. Exact:',\n",
    "        rows=8, # Height of the box\n",
    "        style={'description_width': 'initial'},\n",
    "        layout=widgets.Layout(width='400px')\n",
    "    )\n",
    "    \n",
    "    # Helper text for multi-select\n",
    "    w_multi_help = widgets.HTML(\n",
    "        value=\"<div style='font-size:0.8em; color:#666; margin-left:100px;'><i>Hold <b>Ctrl</b> (Win) or <b>Cmd</b> (Mac) to select multiple.</i></div>\"\n",
    "    )\n",
    "\n",
    "    w_exact_new = widgets.Text(\n",
    "        placeholder='Type NEW keyword here...',\n",
    "        layout=widgets.Layout(width='400px')\n",
    "    )\n",
    "    # Button to add the typed keyword to the list immediately\n",
    "    btn_add_exact = widgets.Button(\n",
    "        description='Add to List',\n",
    "        icon='plus',\n",
    "        layout=widgets.Layout(width='100px')\n",
    "    )\n",
    "\n",
    "    # Info & Display\n",
    "    w_info = widgets.HTML(value=\"<div style='color:#666; font-style:italic; margin-left:100px;'>Select a category...</div>\")\n",
    "    w_entity_display = widgets.HTML()\n",
    "    output_log = widgets.Output()\n",
    "\n",
    "    # Buttons\n",
    "    btn_save = widgets.Button(description='Save & Next ‚û°Ô∏è', button_style='primary')\n",
    "    btn_prev = widgets.Button(description='‚¨ÖÔ∏è Previous')\n",
    "\n",
    "    # --- 3. LOGIC ---\n",
    "\n",
    "    def get_split_history():\n",
    "        \"\"\"Reads all exact categories, splits by ';', and returns unique items for the dropdown.\"\"\"\n",
    "        all_vals = df['qh_exact_category'].dropna().unique()\n",
    "        unique_items = set()\n",
    "        for val in all_vals:\n",
    "            if str(val) == \"nan\" or str(val).strip() == \"\": continue\n",
    "            parts = [p.strip() for p in str(val).split(';')]\n",
    "            for p in parts:\n",
    "                if p: unique_items.add(p)\n",
    "        return sorted(list(unique_items))\n",
    "\n",
    "    def update_display():\n",
    "        \"\"\"Refreshes UI for current row.\"\"\"\n",
    "        if current_idx >= len(df):\n",
    "            w_entity_display.value = \"<div style='background:#d4edda; color:#155724; padding:15px;'><h3>üéâ All Done!</h3></div>\"\n",
    "            return\n",
    "\n",
    "        row = df.loc[current_idx]\n",
    "        \n",
    "        # Display Context\n",
    "        w_entity_display.value = f\"\"\"\n",
    "        <div style=\"background-color: #f8f9fa; border-left: 5px solid #0d6efd; padding: 15px; margin-bottom: 10px;\">\n",
    "            <div style=\"margin-bottom: 5px; font-size: 0.9em; color: #495057;\">\n",
    "                <b>Original Entity:</b> <span style=\"font-family: monospace; font-size: 1.1em;\">{row['entity_name']}</span>\n",
    "            </div>\n",
    "            <div style=\"font-size: 0.85em; color: #666;\">\n",
    "                Doc: {row.get('Document_Name', 'N/A')} | Year: {row.get('Year', 'N/A')}\n",
    "            </div>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Name Edit\n",
    "        existing_final = str(row['finalized_entity_name'])\n",
    "        if existing_final and existing_final.strip() != \"\" and existing_final != \"nan\":\n",
    "            w_name_edit.value = existing_final\n",
    "        else:\n",
    "            w_name_edit.value = str(row['entity_name'])\n",
    "\n",
    "        # Load Categories\n",
    "        current_cat = row['qh_category']\n",
    "        current_sub = row['qh_sub_category']\n",
    "        current_exact = str(row['qh_exact_category'])\n",
    "\n",
    "        # 1. Set Helix\n",
    "        if current_cat in w_helix.options:\n",
    "            w_helix.value = current_cat\n",
    "        elif current_cat:\n",
    "            w_helix.options = list(w_helix.options)[:-1] + [current_cat, NEW_OPT]\n",
    "            w_helix.value = current_cat\n",
    "        else:\n",
    "            w_helix.value = ''\n",
    "\n",
    "        # 2. Set Sub (Trigger updates)\n",
    "        update_sub_options(w_helix.value)\n",
    "        if current_sub in w_sub.options:\n",
    "            w_sub.value = current_sub\n",
    "        elif current_sub:\n",
    "            w_sub.options = list(w_sub.options)[:-1] + [current_sub, NEW_OPT]\n",
    "            w_sub.value = current_sub\n",
    "            \n",
    "        # 3. Set Exact (Trigger updates based on Sub)\n",
    "        update_exact_options(w_helix.value, w_sub.value)\n",
    "        \n",
    "        # Handle Pre-selection of Multiple Items\n",
    "        if current_exact and current_exact != \"nan\" and current_exact.strip():\n",
    "            # Split by semicolon\n",
    "            selected_items = [x.strip() for x in current_exact.split(';')]\n",
    "            # Ensure they exist in options\n",
    "            current_options = list(w_exact_multi.options)\n",
    "            for item in selected_items:\n",
    "                if item not in current_options and item != \"\":\n",
    "                    current_options.append(item)\n",
    "            \n",
    "            w_exact_multi.options = sorted(current_options)\n",
    "            \n",
    "            # Set Value (must be a tuple of matching strings)\n",
    "            valid_selection = [x for x in selected_items if x in w_exact_multi.options]\n",
    "            w_exact_multi.value = tuple(valid_selection)\n",
    "        else:\n",
    "            w_exact_multi.value = ()\n",
    "\n",
    "        # Reset New Fields\n",
    "        w_helix_new.layout.display = 'none'\n",
    "        w_sub_new.layout.display = 'none'\n",
    "        w_exact_new.value = '' # Clear new keyword box\n",
    "\n",
    "        # Progress\n",
    "        done_count = len(df[df['qh_category'] != \"\"])\n",
    "        progress.value = done_count\n",
    "        progress_label.value = f\"{done_count} / {total_items} tagged\"\n",
    "\n",
    "    def update_sub_options(main_cat):\n",
    "        if main_cat in TAXONOMY:\n",
    "            opts = sorted(list(TAXONOMY[main_cat].keys()))\n",
    "            w_sub.options = [''] + opts + [NEW_OPT]\n",
    "            w_sub.disabled = False\n",
    "        elif main_cat and main_cat != NEW_OPT:\n",
    "            w_sub.options = [''] + [NEW_OPT]\n",
    "            w_sub.disabled = False\n",
    "        else:\n",
    "            w_sub.options = []\n",
    "            w_sub.disabled = True\n",
    "\n",
    "    def update_exact_options(main, sub):\n",
    "        \"\"\"Populates SelectMultiple with Keywords + Single Item History\"\"\"\n",
    "        options = []\n",
    "        \n",
    "        # 1. Add Taxonomy Keywords\n",
    "        if main in TAXONOMY and sub in TAXONOMY[main]:\n",
    "            keywords = sorted(TAXONOMY[main][sub].get('keywords', []))\n",
    "            options += keywords\n",
    "        \n",
    "        # 2. Add Global History (Split individual items)\n",
    "        history = get_split_history()\n",
    "        # Merge and Unique\n",
    "        combined = sorted(list(set(options + history)))\n",
    "        \n",
    "        w_exact_multi.options = combined\n",
    "\n",
    "    # --- EVENT HANDLERS ---\n",
    "\n",
    "    def on_helix_change(change):\n",
    "        if change['type'] == 'change' and change['name'] == 'value':\n",
    "            val = change['new']\n",
    "            if val == NEW_OPT:\n",
    "                w_helix_new.layout.display = 'block'\n",
    "                w_sub.options = [''] + [NEW_OPT]\n",
    "                w_sub.disabled = False\n",
    "                w_info.value = \"\"\n",
    "            else:\n",
    "                w_helix_new.layout.display = 'none'\n",
    "                update_sub_options(val)\n",
    "                update_exact_options(val, '')\n",
    "                w_info.value = \"\"\n",
    "\n",
    "    def on_sub_change(change):\n",
    "        if change['type'] == 'change' and change['name'] == 'value':\n",
    "            val = change['new']\n",
    "            main = w_helix.value\n",
    "            \n",
    "            if val == NEW_OPT:\n",
    "                w_sub_new.layout.display = 'block'\n",
    "                w_info.value = \"\"\n",
    "            else:\n",
    "                w_sub_new.layout.display = 'none'\n",
    "                update_exact_options(main, val)\n",
    "                \n",
    "                if main in TAXONOMY and val in TAXONOMY[main]:\n",
    "                    desc = TAXONOMY[main][val]['desc']\n",
    "                    w_info.value = f\"<div style='color:#0d6efd; margin-left:100px;'>‚ÑπÔ∏è {desc}</div>\"\n",
    "\n",
    "    def add_new_keyword(b):\n",
    "        \"\"\"Adds text from input box to the multiple selection list and selects it.\"\"\"\n",
    "        new_val = w_exact_new.value.strip()\n",
    "        if new_val:\n",
    "            # Add to options\n",
    "            current_opts = list(w_exact_multi.options)\n",
    "            if new_val not in current_opts:\n",
    "                current_opts.append(new_val)\n",
    "                w_exact_multi.options = sorted(current_opts)\n",
    "            \n",
    "            # Add to selection\n",
    "            current_sel = list(w_exact_multi.value)\n",
    "            if new_val not in current_sel:\n",
    "                current_sel.append(new_val)\n",
    "                w_exact_multi.value = tuple(current_sel)\n",
    "            \n",
    "            w_exact_new.value = \"\" # Clear input\n",
    "\n",
    "    def save_and_next(b):\n",
    "        global current_idx\n",
    "        \n",
    "        # 1. Name Logic\n",
    "        original_name = str(df.at[current_idx, 'entity_name']).strip()\n",
    "        new_name_input = str(w_name_edit.value).strip()\n",
    "        \n",
    "        if new_name_input != original_name:\n",
    "            df.at[current_idx, 'modified_entity_name'] = True\n",
    "            df.at[current_idx, 'finalized_entity_name'] = new_name_input\n",
    "            name_log = f\"Edited name\"\n",
    "        else:\n",
    "            df.at[current_idx, 'modified_entity_name'] = False\n",
    "            df.at[current_idx, 'finalized_entity_name'] = original_name\n",
    "            name_log = \"Name original\"\n",
    "\n",
    "        # 2. Category Logic\n",
    "        val_cat = w_helix_new.value if w_helix.value == NEW_OPT else w_helix.value\n",
    "        val_sub = w_sub_new.value if w_sub.value == NEW_OPT else w_sub.value\n",
    "        \n",
    "        # Process Exact: Join tuple values with semicolon\n",
    "        selected_exacts = w_exact_multi.value\n",
    "        val_exact = \"; \".join(selected_exacts)\n",
    "        \n",
    "        # Learn Main\n",
    "        if w_helix.value == NEW_OPT and val_cat:\n",
    "            if val_cat not in TAXONOMY:\n",
    "                TAXONOMY[val_cat] = {}\n",
    "                opts = list(w_helix.options)\n",
    "                opts.insert(-1, val_cat)\n",
    "                w_helix.options = opts\n",
    "        \n",
    "        # Learn Sub\n",
    "        if w_sub.value == NEW_OPT and val_sub:\n",
    "            if val_cat not in TAXONOMY: TAXONOMY[val_cat] = {}\n",
    "            if val_sub not in TAXONOMY[val_cat]:\n",
    "                TAXONOMY[val_cat][val_sub] = {'desc': 'User Added', 'keywords': []}\n",
    "            opts = list(w_sub.options)\n",
    "            opts.insert(-1, val_sub)\n",
    "            w_sub.options = opts\n",
    "\n",
    "        # Exact History Learning is handled automatically next time get_split_history is called on the DF\n",
    "\n",
    "        # Save to DF\n",
    "        df.at[current_idx, 'qh_category'] = val_cat\n",
    "        df.at[current_idx, 'qh_sub_category'] = val_sub\n",
    "        df.at[current_idx, 'qh_exact_category'] = val_exact\n",
    "        \n",
    "        df.to_csv(filename, index=False)\n",
    "        \n",
    "        with output_log:\n",
    "            clear_output(wait=True)\n",
    "            print(f\"‚úÖ Saved. ({name_log})\")\n",
    "\n",
    "        # Next\n",
    "        if current_idx < len(df) - 1:\n",
    "            current_idx += 1\n",
    "            update_display()\n",
    "        else:\n",
    "            with output_log:\n",
    "                print(\"End of list reached.\")\n",
    "\n",
    "    def go_prev(b):\n",
    "        global current_idx\n",
    "        if current_idx > 0:\n",
    "            current_idx -= 1\n",
    "            update_display()\n",
    "\n",
    "    # --- 4. LAYOUT ---\n",
    "    w_helix.observe(on_helix_change)\n",
    "    w_sub.observe(on_sub_change)\n",
    "    btn_add_exact.on_click(add_new_keyword)\n",
    "    \n",
    "    btn_save.on_click(save_and_next)\n",
    "    btn_prev.on_click(go_prev)\n",
    "\n",
    "    update_display()\n",
    "\n",
    "    ui = widgets.VBox([\n",
    "        widgets.HBox([progress, progress_label]),\n",
    "        w_entity_display,\n",
    "        \n",
    "        w_name_edit,\n",
    "        widgets.HTML(\"<div style='height:10px;'></div>\"),\n",
    "\n",
    "        w_helix, w_helix_new,\n",
    "        w_sub, w_sub_new,\n",
    "        w_info,\n",
    "        \n",
    "        widgets.HTML(\"<hr style='margin: 5px 0; border:0; border-top:1px solid #eee;'>\"),\n",
    "        \n",
    "        # Exact Multi Select Section\n",
    "        widgets.VBox([\n",
    "            w_exact_multi,\n",
    "            w_multi_help,\n",
    "            widgets.HBox([w_exact_new, btn_add_exact])\n",
    "        ]),\n",
    "        \n",
    "        widgets.HTML(\"<hr style='margin: 10px 0;'>\"),\n",
    "        widgets.HBox([btn_prev, btn_save]),\n",
    "        output_log\n",
    "    ])\n",
    "    \n",
    "    display(ui)\n",
    "\n",
    "else:\n",
    "    print(\"DataFrame is empty.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
