{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "557b3792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning folder: countries_edited...\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'text_countries_edited.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPermissionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[97]\u001b[39m\u001b[32m, line 107\u001b[39m\n\u001b[32m    104\u001b[39m input_folder = \u001b[33m\"\u001b[39m\u001b[33mcountries_edited\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    105\u001b[39m output_filename = \u001b[33m\"\u001b[39m\u001b[33mtext_countries_edited.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m \u001b[43mprocess_folder\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_filename\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[97]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36mprocess_folder\u001b[39m\u001b[34m(folder_path, output_csv)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mScanning folder: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfolder_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Open CSV file once for writing\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput_csv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mw\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m csv_file:\n\u001b[32m     41\u001b[39m     writer = csv.writer(csv_file)\n\u001b[32m     43\u001b[39m     \u001b[38;5;66;03m# Update Header: Added \"Doc_ID\" as the first column\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\OneDrive - TUM\\Master Thesis\\Analysis\\NEUSTART\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:344\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    338\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    339\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    342\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m344\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mPermissionError\u001b[39m: [Errno 13] Permission denied: 'text_countries_edited.csv'"
     ]
    }
   ],
   "source": [
    "# Text Extraction: Final Version with Doc ID\n",
    "\n",
    "import pymupdf\n",
    "import re\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from multi_column import column_boxes\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Bereinigt Text: Fixiert Silbentrennung, Initialen und Whitespace.\n",
    "    \"\"\"\n",
    "\n",
    "    if not text:\n",
    "        return \"\"\n",
    "\n",
    "    # 1. Fix Hyphenation (Word-\\npart -> Wordpart)\n",
    "    text = re.sub(r\"(\\w+)-\\s*\\n\\s*(\\w+)\", r\"\\1\\2\", text)\n",
    "\n",
    "    # 3. Collapse whitespace\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "def process_folder(folder_path: str, output_csv: str):\n",
    "    \"\"\"\n",
    "    Iterates through all PDFs in folder_path, extracts metadata from filenames,\n",
    "    extracts/cleans text from pages, and writes everything to a single CSV.\n",
    "    \"\"\"\n",
    "\n",
    "    pdf_dir = Path(folder_path)\n",
    "    \n",
    "    # Check if directory exists\n",
    "    if not pdf_dir.exists():\n",
    "        print(f\"Directory not found: {folder_path}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Scanning folder: {folder_path}...\")\n",
    "\n",
    "    # Open CSV file once for writing\n",
    "    with open(output_csv, mode='w', newline='', encoding='utf-8') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "\n",
    "        # Update Header: Added \"Doc_ID\" as the first column\n",
    "        writer.writerow([\"Doc_ID\", \"Country\", \"Year\", \"Document_Name\", \"Page\", \"Block_ID\", \"text\"])\n",
    "\n",
    "        # Iterate over all PDF files in the directory\n",
    "        pdf_files = list(pdf_dir.glob(\"*.pdf\"))\n",
    "\n",
    "        if not pdf_files:\n",
    "            print(\"No PDF files found in the directory.\")\n",
    "            return\n",
    "\n",
    "        # Enumerate gives us a counter (doc_id) starting at 0\n",
    "        for doc_id, pdf_path in enumerate(pdf_files, start=0):\n",
    "\n",
    "            # --- Metadata Extraction ---\n",
    "            parts = pdf_path.stem.split(\"_\")\n",
    "            country = parts[0]\n",
    "\n",
    "            # Check if second part is a year (digits)\n",
    "            year = parts[1] if len(parts) > 1 and parts[1].isdigit() else \"\"\n",
    "\n",
    "            # Join the rest as the document name\n",
    "            doc_name = \"_\".join(parts[2:]) if len(parts) > 2 else \"\"\n",
    "\n",
    "            print(f\"Processing ID {doc_id}: {pdf_path.name} | Country: {country}, Year: {year}\")\n",
    "\n",
    "            try:\n",
    "\n",
    "                # --- PDF Text Extraction ---\n",
    "                doc = pymupdf.open(pdf_path)\n",
    "\n",
    "                for page_num, page in enumerate(doc, start=1):\n",
    "\n",
    "                    # Column detection (Bounding Boxes)\n",
    "                    bboxes = column_boxes(page, footer_margin=50, no_image_text=True)\n",
    "\n",
    "                    for block_id, rect in enumerate(bboxes, start=1):\n",
    "\n",
    "                        # Extract text from the specific box\n",
    "                        raw_text = page.get_text(clip=rect, sort=True)\n",
    "\n",
    "                        # Clean text\n",
    "                        final_text = clean_text(raw_text)\n",
    "\n",
    "                        # Write to CSV if text exists\n",
    "                        if final_text:\n",
    "                            # Added doc_id to the row data\n",
    "                            writer.writerow([doc_id, country, year, doc_name, page_num, block_id, final_text])\n",
    "\n",
    "                doc.close()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to process {pdf_path.name}: {e}\")\n",
    "\n",
    "\n",
    "    print(f\"Extraction complete. All data saved in '{output_csv}'.\")\n",
    "\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Define your folder and output filename here\n",
    "    input_folder = \"countries_edited\"\n",
    "    output_filename = \"text_countries_edited.csv\"\n",
    "\n",
    "    process_folder(input_folder, output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c0bea27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Doc_ID Country  Year                       Document_Name  Page  Block_ID  \\\n",
      "0       0     CAN  2022  Canada's National Quantum Strategy     2         1   \n",
      "1       0     CAN  2022  Canada's National Quantum Strategy     3         1   \n",
      "2       0     CAN  2022  Canada's National Quantum Strategy     3         2   \n",
      "3       0     CAN  2022  Canada's National Quantum Strategy     4         1   \n",
      "4       0     CAN  2022  Canada's National Quantum Strategy     4         2   \n",
      "\n",
      "                                                text  \n",
      "0  Executive summary dvances in quantum science h...  \n",
      "1  Three key missions The National Quantum Strate...  \n",
      "2  Next steps To strengthen Canada‚Äôs quantum ecos...  \n",
      "3                          Canada: A quantum pioneer  \n",
      "4  ince the birth of quantum science more than 10...  \n"
     ]
    }
   ],
   "source": [
    "# Load csv to df\n",
    "import pandas as pd\n",
    "output_filename = \"text_countries_edited.csv\"\n",
    "df = pd.read_csv(output_filename)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced14567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "2026-01-14 20:55:39,604 SequenceTagger predicts: Dictionary with 20 tags: <unk>, O, S-ORG, S-MISC, B-PER, E-PER, S-LOC, B-ORG, E-ORG, I-PER, S-PER, B-MISC, I-MISC, E-MISC, I-ORG, B-LOC, E-LOC, I-LOC, <START>, <STOP>\n",
      "Preparing data for batch processing...\n",
      "Total sentences to process: 1359\n",
      "Starting batched prediction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 43/43 [01:43<00:00,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting entities and merging metadata...\n",
      "--- Processing Complete ---\n",
      "Extracted 937 entities.\n",
      "Saved to: entities2.csv\n",
      "    Doc_ID Sentence_ID Country  Year                       Document_Name  \\\n",
      "750      1          10     CAN  2022  Canada's National Quantum Strategy   \n",
      "917      1         102     CAN  2022  Canada's National Quantum Strategy   \n",
      "852      1         111     CAN  2022  Canada's National Quantum Strategy   \n",
      "316      1         118     CAN  2022  Canada's National Quantum Strategy   \n",
      "468      1         118     CAN  2022  Canada's National Quantum Strategy   \n",
      "\n",
      "                     entity_name ner_label qh_category qh_sub_category  \\\n",
      "750         Government of Canada       ORG                               \n",
      "917  Standards Council of Canada       ORG                               \n",
      "852               Bank of Canada       ORG                               \n",
      "316            Quantum Computing       ORG                               \n",
      "468  Innovative Solutions Canada       ORG                               \n",
      "\n",
      "    qh_exact_category  \n",
      "750                    \n",
      "917                    \n",
      "852                    \n",
      "316                    \n",
      "468                    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import flair\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "from segtok.segmenter import split_single\n",
    "import torch\n",
    "\n",
    "# --- 1. SETUP & CONFIGURATION ---\n",
    "\n",
    "# Auto-detect GPU for massive speedup\n",
    "flair.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {flair.device}\")\n",
    "\n",
    "# Load the FAST model (20x faster, ~98% relative accuracy of large model)\n",
    "tagger = SequenceTagger.load('ner-fast')\n",
    "\n",
    "# --- 2. LOAD & PREPARE DATA ---\n",
    "\n",
    "# Ensure text is string and handle missing values\n",
    "# Note: Ensure 'df' is defined before this step (e.g., df = pd.read_csv(...))\n",
    "df[\"text\"] = df[\"text\"].fillna(\"\").astype(str)\n",
    "print(\"Preparing data for batch processing...\")\n",
    "\n",
    "# We need to flatten the data: Documents -> Sentences.\n",
    "# We use 'row_mapping' to remember which original row/block a sentence came from.\n",
    "\n",
    "all_sentences = []\n",
    "row_mapping = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    text = row['text']\n",
    "\n",
    "    # Skip extremely short garbage text\n",
    "    if len(text) < 2:\n",
    "        continue\n",
    "\n",
    "    # Split text into sentences (essential for Flair accuracy)\n",
    "    sentences_raw = split_single(text)\n",
    "\n",
    "    for sent_raw in sentences_raw:\n",
    "        if sent_raw.strip():\n",
    "            sent_obj = Sentence(sent_raw)\n",
    "            all_sentences.append(sent_obj)\n",
    "            row_mapping.append(idx)  # Store the index of the original row\n",
    "\n",
    "print(f\"Total sentences to process: {len(all_sentences)}\")\n",
    "\n",
    "# --- 3. BATCH PREDICTION (High Speed) ---\n",
    "\n",
    "# Batch size: 32 for CPU, 128+ for GPU\n",
    "BATCH_SIZE = 256 if torch.cuda.is_available() else 32\n",
    "\n",
    "print(\"Starting batched prediction...\")\n",
    "tagger.predict(all_sentences, mini_batch_size=BATCH_SIZE, verbose=True)\n",
    "\n",
    "# --- 4. EXTRACT & MERGE METADATA ---\n",
    "\n",
    "print(\"Extracting entities and merging metadata...\")\n",
    "\n",
    "# Set used for deduplication per original block\n",
    "# Stores: (original_index, entity_text, entity_label)\n",
    "unique_entities = set()\n",
    "\n",
    "# Iterate through predictions and map them back to original row index\n",
    "for idx, sent_obj in zip(row_mapping, all_sentences):\n",
    "    for entity in sent_obj.get_spans('ner'):\n",
    "        # Filter for only relevant tags\n",
    "        if entity.tag in ['ORG', 'PER']:\n",
    "            # Add to set (deduplicates if same entity appears twice in same block)\n",
    "            unique_entities.add((idx, entity.text, entity.tag))\n",
    "\n",
    "# --- 5. BUILD FINAL DATAFRAME ---\n",
    "\n",
    "final_rows = []\n",
    "\n",
    "for idx, entity_name, ner_label in unique_entities:\n",
    "    # Retrieve the original row metadata using the index\n",
    "    original_row = df.loc[idx]\n",
    "\n",
    "    final_rows.append({\n",
    "        \"Country\": original_row.get(\"Country\", \"\"),\n",
    "        \"Year\": original_row.get(\"Year\", \"\"),\n",
    "        \"Document_Name\": original_row.get(\"Document_Name\", \"\"),\n",
    "        \"Page\": original_row.get(\"Page\", \"\"),\n",
    "        \"Block_ID\": original_row.get(\"Block_ID\", \"\"),\n",
    "        \"entity_name\": entity_name,\n",
    "        \"ner_label\": ner_label,\n",
    "        \"qh_category\": \"\",        # Empty for manual input\n",
    "        \"qh_sub_category\": \"\",    # Empty for manual input\n",
    "        \"qh_exact_category\": \"\"   # Empty for manual input\n",
    "    })\n",
    "\n",
    "output_df = pd.DataFrame(final_rows)\n",
    "\n",
    "# Sort for easier manual tagging (Group by Document, then Page, then Entity)\n",
    "sort_cols = [c for c in [\"Document_Name\", \"Page\", \"entity_name\"] if c in output_df.columns]\n",
    "if sort_cols:\n",
    "    output_df = output_df.sort_values(by=sort_cols)\n",
    "\n",
    "# --- 6. SAVE TO CSV ---\n",
    "\n",
    "output_filename = \"entities.csv\"\n",
    "output_filename_to_edit = \"entities_to_edit.csv\"\n",
    "\n",
    "output_df.to_csv(output_filename, index=False)\n",
    "output_df.to_csv(output_filename_to_edit, index=False)\n",
    "\n",
    "print(f\"--- Processing Complete ---\")\n",
    "print(f\"Extracted {len(output_df)} entities.\")\n",
    "print(f\"Saved to: {output_filename}\")\n",
    "print(output_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91835832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a640b26cf9324406b1a672f13d4ad8a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=648, bar_style='success', description='Progress:', layout=Layo‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# --- 1. CONFIGURATION & DATA LOADING ---\n",
    "filename = \"2entities_to_edit.csv\"\n",
    "\n",
    "# --- TAXONOMY (General English) ---\n",
    "TAXONOMY = {\n",
    "    \"Academia\": {\n",
    "        \"Higher Education Institutions (HEIs)\": {\n",
    "            \"desc\": \"Universities, Colleges, Schools (Teaching & Research).\",\n",
    "            \"keywords\": [\n",
    "                \"University\", \"College\", \"School\", \"Academy\", \"Faculty\", \"Department\", \n",
    "                \"Chair\", \"Campus\", \"Institute of Technology\", \"Polytechnic\", \n",
    "                \"Business School\", \"Medical School\", \"Law School\"\n",
    "            ]\n",
    "        },\n",
    "        \"Public Research Orgs (PROs)\": {\n",
    "            \"desc\": \"Research Institutes (Knowledge Output, no teaching).\",\n",
    "            \"keywords\": [\n",
    "                \"Institute\", \"Center\", \"Centre\", \"Laboratory\", \"Lab\", \"Observatory\", \n",
    "                \"National Lab\", \"Research Council\", \"Think Tank\", \"Agency (Research)\"\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    \"Industry\": {\n",
    "        \"Private Firms (Corporates)\": {\n",
    "            \"desc\": \"Established Companies, SMEs, MNEs.\",\n",
    "            \"keywords\": [\n",
    "                \"Inc\", \"Corp\", \"Corporation\", \"Ltd\", \"LLC\", \"PLC\", \"Co\", \"Company\", \n",
    "                \"Group\", \"Holdings\", \"Manufacturer\", \"Supplier\", \"Vendor\", \n",
    "                \"Conglomerate\", \"Multinational\", \"Enterprise\", \"Firm\"\n",
    "            ]\n",
    "        },\n",
    "        \"Start-ups\": {\n",
    "            \"desc\": \"Young Growth Companies, Spin-offs.\",\n",
    "            \"keywords\": [\n",
    "                \"Start-up\", \"Startup\", \"Spin-off\", \"Spinoff\", \"Scale-up\", \"Unicorn\", \n",
    "                \"Venture\", \"NewCo\", \"DeepTech\", \"Founder\", \"Stealth Mode\"\n",
    "            ]\n",
    "        },\n",
    "        \"Consulting\": {\n",
    "            \"desc\": \"Services, Advisory, Legal, HR.\",\n",
    "            \"keywords\": [\n",
    "                \"Consulting\", \"Consultancy\", \"Advisors\", \"Partners\", \"Legal\", \"Law Firm\", \n",
    "                \"LLP\", \"Attorney\", \"IP Law\", \"Patent\", \"Audit\", \"Tax\", \"Recruitment\", \n",
    "                \"Headhunter\", \"Strategy\", \"Management\", \"Services\"\n",
    "            ]\n",
    "        },\n",
    "        \"Venture Capital / Investors\": {\n",
    "            \"desc\": \"Financial Actors, VCs, Business Angels.\",\n",
    "            \"keywords\": [\n",
    "                \"Capital\", \"Invest\", \"Investment\", \"Fund\", \"Venture\", \"VC\", \"Equity\", \n",
    "                \"Private Equity\", \"PE\", \"Angel\", \"Seed\", \"Asset Management\", \"Bank\", \n",
    "                \"Financial Group\", \"Holding\", \"Wealth Management\"\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    \"Government\": {\n",
    "        \"Policy Makers\": {\n",
    "            \"desc\": \"Ministries, Councils, Parliaments (Regulation).\",\n",
    "            \"keywords\": [\n",
    "                \"Ministry\", \"Department\", \"Dept\", \"Council\", \"Government\", \"Federal\", \n",
    "                \"State\", \"Municipality\", \"City\", \"County\", \"District\", \"Parliament\", \n",
    "                \"Senate\", \"Commission\", \"Mayor\", \"Governor\", \"Regulator\", \"Authority\", \n",
    "                \"Administration\", \"Bureau\"\n",
    "            ]\n",
    "        },\n",
    "        \"Funding Agencies\": {\n",
    "            \"desc\": \"Funding Bodies, Project Management Agencies.\",\n",
    "            \"keywords\": [\n",
    "                \"Foundation\", \"Agency\", \"Grant\", \"Funding\", \"Fund\", \"Endowment\", \n",
    "                \"Trust\", \"Award\", \"Scholarship\", \"Fellowship\", \"Program\", \"Initiative\"\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    \"Civil Society\": {\n",
    "        \"Media\": {\n",
    "            \"desc\": \"Press, News, Journals.\",\n",
    "            \"keywords\": [\n",
    "                \"News\", \"Journal\", \"Press\", \"Times\", \"Post\", \"Daily\", \"Review\", \n",
    "                \"Magazine\", \"Publisher\", \"Broadcaster\", \"TV\", \"Radio\", \"Podcast\", \n",
    "                \"Blog\", \"Media\", \"Outlet\", \"Chronicle\", \"Gazette\"\n",
    "            ]\n",
    "        },\n",
    "        \"Cultural Institutions\": {\n",
    "            \"desc\": \"Museums, Libraries, Galleries.\",\n",
    "            \"keywords\": [\n",
    "                \"Museum\", \"Library\", \"Gallery\", \"Theater\", \"Opera\", \"Orchestra\", \n",
    "                \"Archive\", \"Collection\", \"Exhibition\", \"Zoo\", \"Botanical Garden\", \n",
    "                \"Science Center\", \"Planetarium\", \"Hall\"\n",
    "            ]\n",
    "        },\n",
    "        \"NGOs / NPOs\": {\n",
    "            \"desc\": \"Non-Profit, Social Goals, Charities.\",\n",
    "            \"keywords\": [\n",
    "                \"Charity\", \"Non-Profit\", \"NPO\", \"NGO\", \"Organization\", \"Society\", \n",
    "                \"Club\", \"Union\", \"Alliance\", \"Federation\", \"Initiative\", \"Philanthropy\", \n",
    "                \"Foundation (Private)\", \"Mission\", \"Relief\"\n",
    "            ]\n",
    "        },\n",
    "        \"Intermediaries\": {\n",
    "            \"desc\": \"Clusters, Hubs, TTOs, Chambers.\",\n",
    "            \"keywords\": [\n",
    "                \"Cluster\", \"Network\", \"Hub\", \"Incubator\", \"Accelerator\", \"TTO\", \n",
    "                \"Technology Transfer\", \"Chamber of Commerce\", \"Trade Union\", \n",
    "                \"Association\", \"Consortium\", \"Standardization\", \"Body\", \"Council (Trade)\"\n",
    "            ]\n",
    "        },\n",
    "        \"Citizens / Users\": {\n",
    "            \"desc\": \"Citizens, Patients, User Groups.\",\n",
    "            \"keywords\": [\n",
    "                \"Community\", \"Group\", \"Public\", \"Citizen\", \"Patient\", \"User\", \n",
    "                \"Resident\", \"Population\", \"Crowd\", \"Forum\", \"Volunteer\", \"Advocacy\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(filename).fillna(\"\")\n",
    "    required_cols = [\"qh_category\", \"qh_sub_category\", \"qh_exact_category\", \"modified_entity_name\", \"finalized_entity_name\"]\n",
    "    for col in required_cols:\n",
    "        if col not in df.columns:\n",
    "            df[col] = \"\"\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: '{filename}' not found.\")\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "# --- 2. WIDGET SETUP ---\n",
    "\n",
    "if not df.empty:\n",
    "    unfinished_indices = df[df['qh_category'] == \"\"].index.tolist()\n",
    "    current_idx = unfinished_indices[0] if unfinished_indices else 0\n",
    "    NEW_OPT = \"+++ Create New +++\"\n",
    "\n",
    "    # Progress Bar\n",
    "    total_items = len(df)\n",
    "    progress = widgets.IntProgress(\n",
    "        value=len(df) - len(unfinished_indices),\n",
    "        min=0,\n",
    "        max=total_items,\n",
    "        description='Progress:',\n",
    "        bar_style='success',\n",
    "        layout=widgets.Layout(width='99%')\n",
    "    )\n",
    "    progress_label = widgets.Label(value=f\"{progress.value} / {total_items} tagged\")\n",
    "\n",
    "    # --- INPUT WIDGETS ---\n",
    "\n",
    "    # 0. Entity Name Editor\n",
    "    w_name_edit = widgets.Text(\n",
    "        description='<b>Edit Name:</b>',\n",
    "        placeholder='Correct the entity name here...',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout=widgets.Layout(width='600px')\n",
    "    )\n",
    "\n",
    "    # 1. Main Category\n",
    "    w_helix = widgets.Dropdown(\n",
    "        options=[''] + list(TAXONOMY.keys()) + [NEW_OPT],\n",
    "        description='1. Helix:',\n",
    "        layout=widgets.Layout(width='400px')\n",
    "    )\n",
    "    w_helix_new = widgets.Text(\n",
    "        placeholder='Type new Helix category...',\n",
    "        layout=widgets.Layout(width='400px', display='none') \n",
    "    )\n",
    "\n",
    "    # 2. Sub Category\n",
    "    w_sub = widgets.Dropdown(\n",
    "        options=[],\n",
    "        description='2. Type:',\n",
    "        layout=widgets.Layout(width='400px'),\n",
    "        disabled=True\n",
    "    )\n",
    "    w_sub_new = widgets.Text(\n",
    "        placeholder='Type new Sub-Category...',\n",
    "        layout=widgets.Layout(width='400px', display='none')\n",
    "    )\n",
    "\n",
    "    # 3. Exact Category (MULTI SELECT)\n",
    "    w_exact_multi = widgets.SelectMultiple(\n",
    "        options=[],\n",
    "        description='3. Exact:',\n",
    "        rows=8, # Height of the box\n",
    "        style={'description_width': 'initial'},\n",
    "        layout=widgets.Layout(width='400px')\n",
    "    )\n",
    "    \n",
    "    # Helper text for multi-select\n",
    "    w_multi_help = widgets.HTML(\n",
    "        value=\"<div style='font-size:0.8em; color:#666; margin-left:100px;'><i>Hold <b>Ctrl</b> (Win) or <b>Cmd</b> (Mac) to select multiple.</i></div>\"\n",
    "    )\n",
    "\n",
    "    w_exact_new = widgets.Text(\n",
    "        placeholder='Type NEW keyword here...',\n",
    "        layout=widgets.Layout(width='400px')\n",
    "    )\n",
    "    # Button to add the typed keyword to the list immediately\n",
    "    btn_add_exact = widgets.Button(\n",
    "        description='Add to List',\n",
    "        icon='plus',\n",
    "        layout=widgets.Layout(width='100px')\n",
    "    )\n",
    "\n",
    "    # Info & Display\n",
    "    w_info = widgets.HTML(value=\"<div style='color:#666; font-style:italic; margin-left:100px;'>Select a category...</div>\")\n",
    "    w_entity_display = widgets.HTML()\n",
    "    output_log = widgets.Output()\n",
    "\n",
    "    # Buttons\n",
    "    btn_save = widgets.Button(description='Save & Next ‚û°Ô∏è', button_style='primary')\n",
    "    btn_prev = widgets.Button(description='‚¨ÖÔ∏è Previous')\n",
    "\n",
    "    # --- 3. LOGIC ---\n",
    "\n",
    "    def get_split_history():\n",
    "        \"\"\"Reads all exact categories, splits by ';', and returns unique items for the dropdown.\"\"\"\n",
    "        all_vals = df['qh_exact_category'].dropna().unique()\n",
    "        unique_items = set()\n",
    "        for val in all_vals:\n",
    "            if str(val) == \"nan\" or str(val).strip() == \"\": continue\n",
    "            parts = [p.strip() for p in str(val).split(';')]\n",
    "            for p in parts:\n",
    "                if p: unique_items.add(p)\n",
    "        return sorted(list(unique_items))\n",
    "\n",
    "    def update_display():\n",
    "        \"\"\"Refreshes UI for current row.\"\"\"\n",
    "        if current_idx >= len(df):\n",
    "            w_entity_display.value = \"<div style='background:#d4edda; color:#155724; padding:15px;'><h3>üéâ All Done!</h3></div>\"\n",
    "            return\n",
    "\n",
    "        row = df.loc[current_idx]\n",
    "        \n",
    "        # Display Context\n",
    "        w_entity_display.value = f\"\"\"\n",
    "        <div style=\"background-color: #f8f9fa; border-left: 5px solid #0d6efd; padding: 15px; margin-bottom: 10px;\">\n",
    "            <div style=\"margin-bottom: 5px; font-size: 0.9em; color: #495057;\">\n",
    "                <b>Original Entity:</b> <span style=\"font-family: monospace; font-size: 1.1em;\">{row['entity_name']}</span>\n",
    "            </div>\n",
    "            <div style=\"font-size: 0.85em; color: #666;\">\n",
    "                Doc: {row.get('Document_Name', 'N/A')} | Year: {row.get('Year', 'N/A')}\n",
    "            </div>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Name Edit\n",
    "        existing_final = str(row['finalized_entity_name'])\n",
    "        if existing_final and existing_final.strip() != \"\" and existing_final != \"nan\":\n",
    "            w_name_edit.value = existing_final\n",
    "        else:\n",
    "            w_name_edit.value = str(row['entity_name'])\n",
    "\n",
    "        # Load Categories\n",
    "        current_cat = row['qh_category']\n",
    "        current_sub = row['qh_sub_category']\n",
    "        current_exact = str(row['qh_exact_category'])\n",
    "\n",
    "        # 1. Set Helix\n",
    "        if current_cat in w_helix.options:\n",
    "            w_helix.value = current_cat\n",
    "        elif current_cat:\n",
    "            w_helix.options = list(w_helix.options)[:-1] + [current_cat, NEW_OPT]\n",
    "            w_helix.value = current_cat\n",
    "        else:\n",
    "            w_helix.value = ''\n",
    "\n",
    "        # 2. Set Sub (Trigger updates)\n",
    "        update_sub_options(w_helix.value)\n",
    "        if current_sub in w_sub.options:\n",
    "            w_sub.value = current_sub\n",
    "        elif current_sub:\n",
    "            w_sub.options = list(w_sub.options)[:-1] + [current_sub, NEW_OPT]\n",
    "            w_sub.value = current_sub\n",
    "            \n",
    "        # 3. Set Exact (Trigger updates based on Sub)\n",
    "        update_exact_options(w_helix.value, w_sub.value)\n",
    "        \n",
    "        # Handle Pre-selection of Multiple Items\n",
    "        if current_exact and current_exact != \"nan\" and current_exact.strip():\n",
    "            # Split by semicolon\n",
    "            selected_items = [x.strip() for x in current_exact.split(';')]\n",
    "            # Ensure they exist in options\n",
    "            current_options = list(w_exact_multi.options)\n",
    "            for item in selected_items:\n",
    "                if item not in current_options and item != \"\":\n",
    "                    current_options.append(item)\n",
    "            \n",
    "            w_exact_multi.options = sorted(current_options)\n",
    "            \n",
    "            # Set Value (must be a tuple of matching strings)\n",
    "            valid_selection = [x for x in selected_items if x in w_exact_multi.options]\n",
    "            w_exact_multi.value = tuple(valid_selection)\n",
    "        else:\n",
    "            w_exact_multi.value = ()\n",
    "\n",
    "        # Reset New Fields\n",
    "        w_helix_new.layout.display = 'none'\n",
    "        w_sub_new.layout.display = 'none'\n",
    "        w_exact_new.value = '' # Clear new keyword box\n",
    "\n",
    "        # Progress\n",
    "        done_count = len(df[df['qh_category'] != \"\"])\n",
    "        progress.value = done_count\n",
    "        progress_label.value = f\"{done_count} / {total_items} tagged\"\n",
    "\n",
    "    def update_sub_options(main_cat):\n",
    "        if main_cat in TAXONOMY:\n",
    "            opts = sorted(list(TAXONOMY[main_cat].keys()))\n",
    "            w_sub.options = [''] + opts + [NEW_OPT]\n",
    "            w_sub.disabled = False\n",
    "        elif main_cat and main_cat != NEW_OPT:\n",
    "            w_sub.options = [''] + [NEW_OPT]\n",
    "            w_sub.disabled = False\n",
    "        else:\n",
    "            w_sub.options = []\n",
    "            w_sub.disabled = True\n",
    "\n",
    "    def update_exact_options(main, sub):\n",
    "        \"\"\"Populates SelectMultiple with Keywords + Single Item History\"\"\"\n",
    "        options = []\n",
    "        \n",
    "        # 1. Add Taxonomy Keywords\n",
    "        if main in TAXONOMY and sub in TAXONOMY[main]:\n",
    "            keywords = sorted(TAXONOMY[main][sub].get('keywords', []))\n",
    "            options += keywords\n",
    "        \n",
    "        # 2. Add Global History (Split individual items)\n",
    "        history = get_split_history()\n",
    "        # Merge and Unique\n",
    "        combined = sorted(list(set(options + history)))\n",
    "        \n",
    "        w_exact_multi.options = combined\n",
    "\n",
    "    # --- EVENT HANDLERS ---\n",
    "\n",
    "    def on_helix_change(change):\n",
    "        if change['type'] == 'change' and change['name'] == 'value':\n",
    "            val = change['new']\n",
    "            if val == NEW_OPT:\n",
    "                w_helix_new.layout.display = 'block'\n",
    "                w_sub.options = [''] + [NEW_OPT]\n",
    "                w_sub.disabled = False\n",
    "                w_info.value = \"\"\n",
    "            else:\n",
    "                w_helix_new.layout.display = 'none'\n",
    "                update_sub_options(val)\n",
    "                update_exact_options(val, '')\n",
    "                w_info.value = \"\"\n",
    "\n",
    "    def on_sub_change(change):\n",
    "        if change['type'] == 'change' and change['name'] == 'value':\n",
    "            val = change['new']\n",
    "            main = w_helix.value\n",
    "            \n",
    "            if val == NEW_OPT:\n",
    "                w_sub_new.layout.display = 'block'\n",
    "                w_info.value = \"\"\n",
    "            else:\n",
    "                w_sub_new.layout.display = 'none'\n",
    "                update_exact_options(main, val)\n",
    "                \n",
    "                if main in TAXONOMY and val in TAXONOMY[main]:\n",
    "                    desc = TAXONOMY[main][val]['desc']\n",
    "                    w_info.value = f\"<div style='color:#0d6efd; margin-left:100px;'>‚ÑπÔ∏è {desc}</div>\"\n",
    "\n",
    "    def add_new_keyword(b):\n",
    "        \"\"\"Adds text from input box to the multiple selection list and selects it.\"\"\"\n",
    "        new_val = w_exact_new.value.strip()\n",
    "        if new_val:\n",
    "            # Add to options\n",
    "            current_opts = list(w_exact_multi.options)\n",
    "            if new_val not in current_opts:\n",
    "                current_opts.append(new_val)\n",
    "                w_exact_multi.options = sorted(current_opts)\n",
    "            \n",
    "            # Add to selection\n",
    "            current_sel = list(w_exact_multi.value)\n",
    "            if new_val not in current_sel:\n",
    "                current_sel.append(new_val)\n",
    "                w_exact_multi.value = tuple(current_sel)\n",
    "            \n",
    "            w_exact_new.value = \"\" # Clear input\n",
    "\n",
    "    def save_and_next(b):\n",
    "        global current_idx\n",
    "        \n",
    "        # 1. Name Logic\n",
    "        original_name = str(df.at[current_idx, 'entity_name']).strip()\n",
    "        new_name_input = str(w_name_edit.value).strip()\n",
    "        \n",
    "        if new_name_input != original_name:\n",
    "            df.at[current_idx, 'modified_entity_name'] = True\n",
    "            df.at[current_idx, 'finalized_entity_name'] = new_name_input\n",
    "            name_log = f\"Edited name\"\n",
    "        else:\n",
    "            df.at[current_idx, 'modified_entity_name'] = False\n",
    "            df.at[current_idx, 'finalized_entity_name'] = original_name\n",
    "            name_log = \"Name original\"\n",
    "\n",
    "        # 2. Category Logic\n",
    "        val_cat = w_helix_new.value if w_helix.value == NEW_OPT else w_helix.value\n",
    "        val_sub = w_sub_new.value if w_sub.value == NEW_OPT else w_sub.value\n",
    "        \n",
    "        # Process Exact: Join tuple values with semicolon\n",
    "        selected_exacts = w_exact_multi.value\n",
    "        val_exact = \"; \".join(selected_exacts)\n",
    "        \n",
    "        # Learn Main\n",
    "        if w_helix.value == NEW_OPT and val_cat:\n",
    "            if val_cat not in TAXONOMY:\n",
    "                TAXONOMY[val_cat] = {}\n",
    "                opts = list(w_helix.options)\n",
    "                opts.insert(-1, val_cat)\n",
    "                w_helix.options = opts\n",
    "        \n",
    "        # Learn Sub\n",
    "        if w_sub.value == NEW_OPT and val_sub:\n",
    "            if val_cat not in TAXONOMY: TAXONOMY[val_cat] = {}\n",
    "            if val_sub not in TAXONOMY[val_cat]:\n",
    "                TAXONOMY[val_cat][val_sub] = {'desc': 'User Added', 'keywords': []}\n",
    "            opts = list(w_sub.options)\n",
    "            opts.insert(-1, val_sub)\n",
    "            w_sub.options = opts\n",
    "\n",
    "        # Exact History Learning is handled automatically next time get_split_history is called on the DF\n",
    "\n",
    "        # Save to DF\n",
    "        df.at[current_idx, 'qh_category'] = val_cat\n",
    "        df.at[current_idx, 'qh_sub_category'] = val_sub\n",
    "        df.at[current_idx, 'qh_exact_category'] = val_exact\n",
    "        \n",
    "        df.to_csv(filename, index=False)\n",
    "        \n",
    "        with output_log:\n",
    "            clear_output(wait=True)\n",
    "            print(f\"‚úÖ Saved. ({name_log})\")\n",
    "\n",
    "        # Next\n",
    "        if current_idx < len(df) - 1:\n",
    "            current_idx += 1\n",
    "            update_display()\n",
    "        else:\n",
    "            with output_log:\n",
    "                print(\"End of list reached.\")\n",
    "\n",
    "    def go_prev(b):\n",
    "        global current_idx\n",
    "        if current_idx > 0:\n",
    "            current_idx -= 1\n",
    "            update_display()\n",
    "\n",
    "    # --- 4. LAYOUT ---\n",
    "    w_helix.observe(on_helix_change)\n",
    "    w_sub.observe(on_sub_change)\n",
    "    btn_add_exact.on_click(add_new_keyword)\n",
    "    \n",
    "    btn_save.on_click(save_and_next)\n",
    "    btn_prev.on_click(go_prev)\n",
    "\n",
    "    update_display()\n",
    "\n",
    "    ui = widgets.VBox([\n",
    "        widgets.HBox([progress, progress_label]),\n",
    "        w_entity_display,\n",
    "        \n",
    "        w_name_edit,\n",
    "        widgets.HTML(\"<div style='height:10px;'></div>\"),\n",
    "\n",
    "        w_helix, w_helix_new,\n",
    "        w_sub, w_sub_new,\n",
    "        w_info,\n",
    "        \n",
    "        widgets.HTML(\"<hr style='margin: 5px 0; border:0; border-top:1px solid #eee;'>\"),\n",
    "        \n",
    "        # Exact Multi Select Section\n",
    "        widgets.VBox([\n",
    "            w_exact_multi,\n",
    "            w_multi_help,\n",
    "            widgets.HBox([w_exact_new, btn_add_exact])\n",
    "        ]),\n",
    "        \n",
    "        widgets.HTML(\"<hr style='margin: 10px 0;'>\"),\n",
    "        widgets.HBox([btn_prev, btn_save]),\n",
    "        output_log\n",
    "    ])\n",
    "    \n",
    "    display(ui)\n",
    "\n",
    "else:\n",
    "    print(\"DataFrame is empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12349687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run again deduplication and save\n",
    "#TODO Run again deduplication and save: since modified_entities might have created duplicates\n",
    "# Eigentlich ja noch Entities und deren Abk√ºrzungen zusammenfassen sonst gibt es ja bei Entities mit Abk√ºrzungen doppelte Eintr√§ge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1fa046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Actor Occurrences: 161\n",
      "  Doc_ID                                Label    qh_category\n",
      "0      0                             Acountry  not specified\n",
      "1      0        The National Quantum Strategy   not an actor\n",
      "2      0                                  NQS   not an actor\n",
      "3      0                 Government of Canada     Government\n",
      "4      0  National Research Council of Canada       Academia\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Deduplication and Final Data Prep\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Load the manually edited file\n",
    "df_edited = pd.read_csv(\"2entities_to_edit.csv\", dtype=str).fillna(\"\")\n",
    "\n",
    "# 2. Filter for valid entities only\n",
    "# We only want rows where a category has been assigned (Human verified)\n",
    "df_final = df_edited[df_edited['qh_category'] != \"\"].copy()\n",
    "\n",
    "# 3. Standardize Names\n",
    "# Use 'finalized_entity_name' as the source of truth. \n",
    "# If it's empty (no edit), fall back to 'entity_name'.\n",
    "df_final['Label'] = df_final['finalized_entity_name'].where(\n",
    "    df_final['finalized_entity_name'] != \"\", \n",
    "    df_final['entity_name']\n",
    ")\n",
    "\n",
    "# 4. Drop Duplicates within the same Context (Block)\n",
    "# If \"Canada\" appears twice in Block 1, we only need it once for the network.\n",
    "df_unique_context = df_final.drop_duplicates(subset=['Doc_ID', 'Block_ID', 'Label'])\n",
    "\n",
    "print(f\"Unique Actor Occurrences: {len(df_unique_context)}\")\n",
    "print(df_unique_context[['Doc_ID', 'Label', 'qh_category']].head())\n",
    "\n",
    "# Save this for the network step\n",
    "df_unique_context.to_csv(\"entities_ready_for_network.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df07afd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "not an actor: absolute number of mentions: 66\n",
      "  -> not an actor (66 mentions)\n",
      "    -> Exact categories: EU (2), Financial Plan (1), Financial Vehicle (2), Fund (3), Funding (3), Grant Program (1), Infrastructure Initiative (1), Mission (1), Policy Framework (2), Program (23), Strategic Document (9), country (1), not an actor (58), not specified (1)\n",
      "\n",
      "Government: absolute number of mentions: 82\n",
      "  -> Bank (3 mentions)\n",
      "    -> Exact categories: Corporate VC (3), Funding (3)\n",
      "  -> Federal Agency (3 mentions)\n",
      "    -> Exact categories: Europe (1), Federal Agency (3), International Standard Body (1), Regulatory Authority (2), Space Agency (1), US (2)\n",
      "  -> Federal Research Organization (2 mentions)\n",
      "    -> Exact categories: Federal Research Agency (2)\n",
      "  -> Funding Agencies (28 mentions)\n",
      "    -> Exact categories: Advisory Service (1), Federal Agency (11), Funding (12), Grant (2), International Partnership (6), Research Council (4), SME Support (1), US (1)\n",
      "  -> Governent Support Service (1 mentions)\n",
      "    -> Exact categories: Federal Agency (1)\n",
      "  -> Government Support Services (5 mentions)\n",
      "    -> Exact categories: Federal Agency (5)\n",
      "  -> Policy Makers (39 mentions)\n",
      "    -> Exact categories: Administrative Unit (1), Advisory Service (2), Armed Forces (1), Central Bank (1), Commission (1), Council (1), EU (1), Federal Agency (2), Federal Department (23), Federal Security (2), Foundation (1), Government (2), Intelligence Authority (2), Military (2), Ministry (19), Monetary Authority (1), Political Union (2), Regulatory Authority (4), Research Council (1), Space Agency (2), Supranational Organization (3)\n",
      "\n",
      "Academia: absolute number of mentions: 20\n",
      "  -> Higher Education Institutions (HEIs) (5 mentions)\n",
      "    -> Exact categories: University (5)\n",
      "  -> Public Research Orgs (PROs) (15 mentions)\n",
      "    -> Exact categories: Institute (11), National Lab (1), National Research Center (2), Research Council (2), Scientific infrastructure provider (1), Space Agency (2), Think Tank (1)\n",
      "\n",
      "Industry: absolute number of mentions: 13\n",
      "  -> Private Firms (Corporates) (5 mentions)\n",
      "    -> Exact categories: Innovation Hub (1), MNE (2), Technology Provider (4)\n",
      "  -> Start-ups (5 mentions)\n",
      "    -> Exact categories: Corporate Spin-Off (1), Subsidiary (1), Technology Provider (4)\n",
      "  -> Venture Capital / Investors (3 mentions)\n",
      "    -> Exact categories: Corporate VC (2), Fund (2), Investment (2), Public-Private Partnership Fund (1), VC (2)\n",
      "\n",
      "Civil Society: absolute number of mentions: 11\n",
      "  -> Intermediaries (11 mentions)\n",
      "    -> Exact categories: Accelerator (1), Alliance (1), Cluster (3), Collaborative Research Center (2), Consortium (4), EU (1), Foundation (1), Funding (1), Hub (3), Incubator (1), Industry Association (1), Innovation Hub (4), Network (4), Standardization (3), Talent & Skills Organization (1), Technical Committee (1), Think Tank (1), funded by the Government (1)\n",
      "\n",
      "not specified: absolute number of mentions: 10\n",
      "  -> not specified (10 mentions)\n",
      "    -> Exact categories: not specified (10)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('2entities_to_edit.csv')\n",
    "\n",
    "# 1. Explode the 'qh_exact_category' column \n",
    "# This splits strings like \"Accelerator; Alliance\" into separate rows so we can count them\n",
    "df_exploded = df.copy()\n",
    "df_exploded['qh_exact_category'] = df_exploded['qh_exact_category'].fillna('not specified').astype(str).str.split(';')\n",
    "df_exploded = df_exploded.explode('qh_exact_category')\n",
    "df_exploded['qh_exact_category'] = df_exploded['qh_exact_category'].str.strip()\n",
    "\n",
    "# 2. Group the data for the hierarchy\n",
    "# We use the original df for top-level counts and exploded df for exact category counts\n",
    "final_counts = df_exploded.groupby(['qh_category', 'qh_sub_category', 'qh_exact_category']).size().reset_index(name='exact_count')\n",
    "\n",
    "# 3. Print the Hierarchical Structure\n",
    "unique_categories = df['qh_category'].dropna().unique()\n",
    "\n",
    "for cat in unique_categories:\n",
    "    # Top Level: Category\n",
    "    cat_total = df[df['qh_category'] == cat].shape[0]\n",
    "    print(f\"\\n{cat}: absolute number of mentions: {cat_total}\")\n",
    "    \n",
    "    # Second Level: Sub-category\n",
    "    sub_df = final_counts[final_counts['qh_category'] == cat]\n",
    "    for sub in sub_df['qh_sub_category'].unique():\n",
    "        sub_total = df[(df['qh_category'] == cat) & (df['qh_sub_category'] == sub)].shape[0]\n",
    "        print(f\"  -> {sub} ({sub_total} mentions)\")\n",
    "        \n",
    "        # Third Level: Exact categories\n",
    "        exact_df = sub_df[sub_df['qh_sub_category'] == sub]\n",
    "        # Format the exact categories into a readable string\n",
    "        exact_list = [f\"{row['qh_exact_category']} ({row['exact_count']})\" for _, row in exact_df.iterrows()]\n",
    "        print(f\"    -> Exact categories: {', '.join(exact_list)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
