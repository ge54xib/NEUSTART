{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc8beb62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up cleaning tools...\n",
      "Loading output.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-20 11:37:09,785 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 528 documents...\n",
      "Fitting Hyper-Granular Model (this may generate hundreds of topics)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1d10a87277e44099c8d7c9bb1edf62e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-20 11:37:22,456 - BERTopic - Embedding - Completed ✓\n",
      "2026-01-20 11:37:22,458 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-20 11:37:35,415 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-20 11:37:35,417 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-01-20 11:37:35,513 - BERTopic - Cluster - Completed ✓\n",
      "2026-01-20 11:37:35,515 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-20 11:37:35,641 - BERTopic - Representation - Completed ✓\n",
      "100%|██████████| 1/1 [00:00<00:00, 82.56it/s]\n",
      "2026-01-20 11:37:35,978 - BERTopic - WARNING: Using a custom list of topic assignments may lead to errors if topic reduction techniques are used afterwards. Make sure that manually assigning topics is the last step in the pipeline.Note that topic embeddings will also be created through weightedc-TF-IDF embeddings instead of centroid embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outliers reduced successfully.\n",
      "\n",
      "Total Topics Found: 108\n",
      "Saved to 'hyper_granular_topics.csv'\n",
      "Visualization saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from hdbscan import HDBSCAN\n",
    "from umap import UMAP\n",
    "\n",
    "# --- 1. PREPROCESSING SETUP ---\n",
    "print(\"Setting up cleaning tools...\")\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "custom_stops = {'page', 'http', 'https', 'www', 'com', 'paragraph', 'table', 'figure', 'section'} \n",
    "stop_words.update(custom_stops)\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    # Lowercase & remove non-text characters\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # Tokenize, remove stopwords, lemmatize\n",
    "    words = [lemmatizer.lemmatize(w) for w in text.split() if w not in stop_words and len(w) > 2]\n",
    "    return \" \".join(words)\n",
    "\n",
    "# --- 2. LOAD DATA ---\n",
    "csv_path = \"output.csv\"\n",
    "print(f\"Loading {csv_path}...\")\n",
    "df = pd.read_csv(csv_path)\n",
    "df = df.dropna(subset=['text'])\n",
    "\n",
    "# Apply cleaning\n",
    "df['clean_text'] = df['text'].apply(clean_text)\n",
    "df = df[df['clean_text'].str.len() > 5] # Remove empty results\n",
    "docs = df['clean_text'].tolist()\n",
    "\n",
    "print(f\"Processing {len(docs)} documents...\")\n",
    "\n",
    "# --- 3. CONFIGURE \"HYPER-GRANULAR\" SETTINGS ---\n",
    "\n",
    "# UMAP: Local Focus\n",
    "# n_neighbors=2 forces the model to only look at very immediate similarities.\n",
    "# This prevents it from grouping vaguely similar ideas together.\n",
    "umap_model = UMAP(\n",
    "    n_neighbors=2,       # EXTREME GRANULARITY (Default is 15)\n",
    "    n_components=5, \n",
    "    min_dist=0.0, \n",
    "    metric='cosine'\n",
    ")\n",
    "\n",
    "# HDBSCAN: Tiny Clusters\n",
    "# min_cluster_size=3 means a topic can be created with just 3 paragraphs.\n",
    "# min_samples=1 ensures almost NO data is treated as noise/outliers.\n",
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size=3,  # EXTREME GRANULARITY (Default is 10)\n",
    "    min_samples=1,       # PREVENTS OUTLIERS (Default is usually larger)\n",
    "    metric='euclidean', \n",
    "    cluster_selection_method='eom', \n",
    "    prediction_data=True\n",
    ")\n",
    "\n",
    "# Vectorizer: Allow rare words to define topics\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\", min_df=2)\n",
    "\n",
    "# --- 4. RUN MODEL ---\n",
    "print(\"Fitting Hyper-Granular Model (this may generate hundreds of topics)...\")\n",
    "\n",
    "topic_model = BERTopic(\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    language=\"english\",\n",
    "    verbose=True,\n",
    "    nr_topics=None # CRITICAL: Do not merge topics\n",
    ")\n",
    "\n",
    "topics, probs = topic_model.fit_transform(docs)\n",
    "\n",
    "# --- 5. REDUCE REMAINING OUTLIERS ---\n",
    "# With min_samples=1, outliers should be low, but this forces 100% assignment\n",
    "try:\n",
    "    new_topics = topic_model.reduce_outliers(docs, topics)\n",
    "    topic_model.update_topics(docs, topics=new_topics)\n",
    "    print(\"Outliers reduced successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"Skipping outlier reduction (likely almost 0 outliers exists).\")\n",
    "\n",
    "# --- 6. EXPORT RESULTS ---\n",
    "freq = topic_model.get_topic_info()\n",
    "print(f\"\\nTotal Topics Found: {len(freq) - 1}\")\n",
    "\n",
    "# Export to CSV\n",
    "freq.to_csv(\"hyper_granular_topics.csv\", index=False)\n",
    "print(\"Saved to 'hyper_granular_topics.csv'\")\n",
    "\n",
    "# Visualize top 50 (since you will have many)\n",
    "fig = topic_model.visualize_barchart(top_n_topics=50)\n",
    "fig.write_html(\"hyper_granular_chart.html\")\n",
    "print(\"Visualization saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2d0842c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Exported 109 topics to 'all_topics_list.csv'.\n",
      "\n",
      "--- ALL TOPICS ---\n",
      "     Topic  Count  \\\n",
      "0        0     10   \n",
      "1        1     10   \n",
      "2        2      9   \n",
      "3        3      9   \n",
      "4        4      8   \n",
      "5        5      8   \n",
      "6        6      9   \n",
      "7        7      8   \n",
      "8        8      8   \n",
      "9        9      7   \n",
      "10      10      7   \n",
      "11      11      7   \n",
      "12      12      7   \n",
      "13      13      8   \n",
      "14      14      6   \n",
      "15      15      6   \n",
      "16      16      6   \n",
      "17      17      6   \n",
      "18      18      6   \n",
      "19      19      6   \n",
      "20      20      6   \n",
      "21      21      6   \n",
      "22      22      6   \n",
      "23      23      6   \n",
      "24      24      6   \n",
      "25      25      6   \n",
      "26      26      5   \n",
      "27      27      5   \n",
      "28      28      5   \n",
      "29      29      5   \n",
      "30      30      5   \n",
      "31      31      5   \n",
      "32      32      5   \n",
      "33      33      5   \n",
      "34      34      5   \n",
      "35      35      5   \n",
      "36      36      5   \n",
      "37      37      5   \n",
      "38      38      5   \n",
      "39      39      7   \n",
      "40      40      6   \n",
      "41      41      5   \n",
      "42      42      5   \n",
      "43      43      5   \n",
      "44      44      6   \n",
      "45      45      6   \n",
      "46      46      7   \n",
      "47      47      5   \n",
      "48      48      5   \n",
      "49      49      4   \n",
      "50      50      4   \n",
      "51      51      4   \n",
      "52      52      4   \n",
      "53      53      4   \n",
      "54      54      4   \n",
      "55      55      4   \n",
      "56      56      4   \n",
      "57      57      4   \n",
      "58      58      5   \n",
      "59      59      4   \n",
      "60      60      4   \n",
      "61      61      4   \n",
      "62      62      4   \n",
      "63      63      4   \n",
      "64      64      4   \n",
      "65      65      4   \n",
      "66      66      4   \n",
      "67      67      4   \n",
      "68      68      4   \n",
      "69      69      4   \n",
      "70      70      4   \n",
      "71      71      4   \n",
      "72      72      4   \n",
      "73      73      4   \n",
      "74      74      4   \n",
      "75      75      4   \n",
      "76      76      4   \n",
      "77      77      4   \n",
      "78      78      4   \n",
      "79      79      4   \n",
      "80      80      4   \n",
      "81      81      4   \n",
      "82      82      4   \n",
      "83      83      4   \n",
      "84      84      3   \n",
      "85      85      3   \n",
      "86      86      3   \n",
      "87      87      3   \n",
      "88      88      3   \n",
      "89      89      3   \n",
      "90      90      3   \n",
      "91      91      3   \n",
      "92      92      5   \n",
      "93      93      3   \n",
      "94      94      3   \n",
      "95      95      3   \n",
      "96      96      3   \n",
      "97      97      3   \n",
      "98      98      3   \n",
      "99      99      3   \n",
      "100    100      3   \n",
      "101    101      3   \n",
      "102    102      3   \n",
      "103    103      3   \n",
      "104    104      3   \n",
      "105    105      3   \n",
      "106    106      3   \n",
      "107    107      3   \n",
      "108    108      4   \n",
      "\n",
      "                                                                     Name  \n",
      "0                                  0_canada_stakeholder_granting_canadian  \n",
      "1                                    1_arrangement_whilst_bilateral_epsrc  \n",
      "2                                       2_computer_computing_qubits_could  \n",
      "3                            3_international_joint_collaboration_deeptech  \n",
      "4                        4_inthe_umbrella_fraunhofergesellschaft_contract  \n",
      "5                                   5_prime_minister_chaired_subcommittee  \n",
      "6                   6_education_acceleration_quantummachine_researchareas  \n",
      "7                                   7_bmf_wellnetworked_bmi_communication  \n",
      "8                                       8_talent_student_program_graduate  \n",
      "9                                         9_defense_mod_military_security  \n",
      "10         10_fraunhofergesellschaft_computer_optimisation_pharmaceutical  \n",
      "11                                    11_sensor_device_sensitivity_timing  \n",
      "12                                      12_arrival_prepare_share_concrete  \n",
      "13                      13_scotland_facility_semiconductor_infrastructure  \n",
      "14                                    14_business_overseas_help_available  \n",
      "15               15_qubits_ondifferent_internationaldevelopments_suitable  \n",
      "16                                16_canadian_transformative_canada_clean  \n",
      "17                           17_privacy_derisk_quantumenabled_postquantum  \n",
      "18                   18_infrastructure_fabrication_requirement_commission  \n",
      "19                                      19_workforce_stem_quarter_diverse  \n",
      "20                                 20_enduser_agency_facility_cultivating  \n",
      "21                                   21_economic_seven_region_immigration  \n",
      "22                             22_source_frequency_entangled_cryptography  \n",
      "23                      23_quality_assurance_characterisation_calibration  \n",
      "24                                            24_pillar_three_mission_nqs  \n",
      "25                                         25_alliance_seven_million_year  \n",
      "26                                 26_software_first_commercial_dedicated  \n",
      "27                                      27_esdc_talent_foreign_designated  \n",
      "28                           28_network_classification_consistent_largest  \n",
      "29                                    29_germany_targeted_technology_term  \n",
      "30                                    30_clock_galileo_generation_optical  \n",
      "31            31_infrastructure_needed_includingcontinued_nationalquantum  \n",
      "32                        32_migration_ingermany_postquantum_cryptography  \n",
      "33                                     33_making_goto_preferred_promotion  \n",
      "34                         34_fraunhofer_cap_fraunhofergesellschaft_laser  \n",
      "35                                      35_risk_give_predictable_security  \n",
      "36                                 36_measure_strategy_federal_conceptual  \n",
      "37                         37_testbeds_ofnorth_ofa_establishingregulatory  \n",
      "38                       38_cooperation_likeminded_geopolitical_billionin  \n",
      "39                                         39_canadian_canada_nqs_program  \n",
      "40                                         40_helmholtz_hub_centre_motion  \n",
      "41                                41_regulatory_regulation_ethical_debate  \n",
      "42                         42_leibniz_standardisation_institute_component  \n",
      "43                                    43_ethical_protects_andthe_champion  \n",
      "44                              44_attract_retain_proactively_engineering  \n",
      "45                       45_outside_communicating_reluctant_relatedfields  \n",
      "46                46_usersof_beingactively_codesigned_conceptualframework  \n",
      "47        47_collaboration_scienceand_betweenindustry_cooperativeresearch  \n",
      "48                                          48_nqcc_sparq_access_resource  \n",
      "49                            49_garching_stuttgart_erlangen_halledresden  \n",
      "50                                       50_qi_grand_scientific_challenge  \n",
      "51                     51_skill_apprenticeship_stem_researchersinnovators  \n",
      "52                                      52_mobility_energy_change_climate  \n",
      "53                              53_doctoral_fellowship_training_increased  \n",
      "54                                            54_catalyst_isc_call_public  \n",
      "55                                  55_cfi_foundation_spectrum_innovation  \n",
      "56                                        56_nqs_roadmaps_mission_council  \n",
      "57                                    57_held_collective_ally_postquantum  \n",
      "58                                   58_provincial_alberta_million_quebec  \n",
      "59       59_communicationlinks_andeurope_publicauthority_quantumrepeaters  \n",
      "60   60_thefraunhofergesellschaft_andexisting_qualifies_technicalsciences  \n",
      "61                                   61_convene_connect_increase_provider  \n",
      "62      62_oinvestment_computingcapabilities_andcompanies_centreincluding  \n",
      "63                                      63_forum_nato_oecd_examplethrough  \n",
      "64                                   64_artificial_nsf_intelligence_nserc  \n",
      "65                          65_iseds_productionof_commercialization_seven  \n",
      "66                                     66_mission_pnt_navigation_focussed  \n",
      "67                              67_second_attain_threewithin_togetherwith  \n",
      "68                              68_programming_related_inexpensive_simple  \n",
      "69                                   69_fieldtested_prototype_easily_back  \n",
      "70                                         70_usd_component_usa_metrology  \n",
      "71                71_andquantumenabled_quantumsecured_inenergy_networksto  \n",
      "72                                               72_bsi_project_cyber_qkd  \n",
      "73                                   73_worker_faster_highskilled_skilled  \n",
      "74                             74_proficiency_language_applicant_official  \n",
      "75                        75_school_theoretical_outreach_andinternational  \n",
      "76                                76_concept_conceptual_framework_measure  \n",
      "77                                       77_prevent_close_european_europe  \n",
      "78                                      78_overseas_business_want_company  \n",
      "79                                     79_priority_standard_report_action  \n",
      "80                                  80_single_optical_atroom_easytohandle  \n",
      "81                              81_skilled_professional_vocational_worker  \n",
      "82                       82_testing_bridge_intodefence_quantummeasurement  \n",
      "83                                  83_ecosystemis_contract_awarding_dlrs  \n",
      "84                                  84_spinoffs_feedback_suggested_talent  \n",
      "85                                   85_nserc_programming_grant_supported  \n",
      "86                                      86_little_relatively_connect_lack  \n",
      "87                                      87_npl_assurance_standard_quantum  \n",
      "88                       88_postquantum_secure_cryptography_communication  \n",
      "89                                     89_adopter_developer_sensing_early  \n",
      "90                                   90_country_billion_worldclass_canada  \n",
      "91                                      91_nrc_assistance_throughput_irap  \n",
      "92                         92_sovereignty_technological_germany_conducive  \n",
      "93                                          93_skill_shortage_talent_much  \n",
      "94                   94_qualification_independent_absence_emergingquantum  \n",
      "95                  95_diagnose_heidelberggarching_inrevenue_onlargescale  \n",
      "96                                              96_mpg_qst_ptb_initiative  \n",
      "97                  97_analysisof_cyberthreats_effectbased_examplequantum  \n",
      "98                                            98_dfg_bwi_european_project  \n",
      "99                                     99_startup_excellent_thereare_itto  \n",
      "100             100_asworldleading_correctionand_nationwide_withcompanies  \n",
      "101                             101_community_coordinate_foster_consensus  \n",
      "102                                        102_cluster_supply_chain_nssif  \n",
      "103             103_formutual_existingprogramme_collaboration_bilaterally  \n",
      "104           104_acceleration_bebetter_byfocussing_programmerequirements  \n",
      "105                               105_programme_specific_existing_current  \n",
      "106                                    106_set_chapter_telecom_superpower  \n",
      "107                                    107_route_visa_individual_talented  \n",
      "108                                         108_nist_programme_sector_nsf  \n"
     ]
    }
   ],
   "source": [
    "# --- 5. Get All Topic Information ---\n",
    "\n",
    "# Get the full dataframe of topic info\n",
    "topic_info = topic_model.get_topic_info()\n",
    "\n",
    "# Method A: Save to CSV (Recommended for viewing all topics easily)\n",
    "topic_info.to_csv(\"all_topics_list.csv\", index=False)\n",
    "print(f\"Success! Exported {len(topic_info)} topics to 'all_topics_list.csv'.\")\n",
    "\n",
    "# Method B: Print all topics to the console\n",
    "# We change pandas settings to ensure rows aren't hidden\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "print(\"\\n--- ALL TOPICS ---\")\n",
    "print(topic_info[['Topic', 'Count', 'Name']])\n",
    "\n",
    "# Reset pandas options (optional)\n",
    "pd.reset_option('display.max_rows')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
