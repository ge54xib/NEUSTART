{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e423594b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot set gray stroke color because /'P0' is an invalid float value\n",
      "Cannot set gray stroke color because /'P0' is an invalid float value\n",
      "Cannot set gray stroke color because /'P0' is an invalid float value\n",
      "Cannot set gray stroke color because /'P0' is an invalid float value\n",
      "Cannot set gray stroke color because /'P0' is an invalid float value\n",
      "Cannot set gray stroke color because /'P0' is an invalid float value\n",
      "Cannot set gray stroke color because /'P0' is an invalid float value\n",
      "Cannot set gray stroke color because /'P0' is an invalid float value\n",
      "Cannot set gray stroke color because /'P0' is an invalid float value\n",
      "Cannot set gray stroke color because /'P0' is an invalid float value\n",
      "Cannot set gray stroke color because /'P0' is an invalid float value\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Cannot set gray stroke color because /'P44' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P44' is an invalid float value\n",
      "Cannot set gray stroke color because /'P61' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P61' is an invalid float value\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Cannot set gray stroke color because /'P67' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P67' is an invalid float value\n"
     ]
    }
   ],
   "source": [
    "#Extract text and metadata from PDF files in a directory and compile into a CSV file.\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "#Iterate over all PDF files in the directory\n",
    "PDF_DIR = Path(\"countries\")\n",
    "OUTPUT_CSV = \"document.csv\"\n",
    "rows = []\n",
    "for pdf_path in PDF_DIR.glob(\"*.pdf\"):\n",
    "    all_text = []\n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text = page.extract_text()\n",
    "            if text:\n",
    "                all_text.append(text)\n",
    "\n",
    "    full_text = \"\\n\".join(all_text)\n",
    "\n",
    "#Extract metadata from filename\n",
    "    stem = pdf_path.stem              \n",
    "    parts = stem.split(\"_\")\n",
    "\n",
    "    country = parts[0] if len(parts) > 0 else None\n",
    "    year = parts[1] if len(parts) > 1 and parts[1].isdigit() else None\n",
    "    strategy_name = \"_\".join(parts[2:]) if len(parts) > 2 else None\n",
    "\n",
    "    rows.append({\n",
    "        \"doc_id\": f\"{country}_{year}\" if year else country,\n",
    "        \"country\": country,\n",
    "        \"year\": year,\n",
    "        \"strategy_name\": strategy_name,\n",
    "        \"file_name\": pdf_path.name,\n",
    "        \"text\": full_text\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# Convert year to numeric, setting errors to NaN for non-numeric values\n",
    "df[\"year\"] = pd.to_numeric(df[\"year\"], errors=\"coerce\")\n",
    "\n",
    "df.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f75ae67a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot set gray stroke color because /'P0' is an invalid float value\n",
      "Cannot set gray stroke color because /'P0' is an invalid float value\n",
      "Cannot set gray stroke color because /'P0' is an invalid float value\n",
      "Cannot set gray stroke color because /'P0' is an invalid float value\n",
      "Cannot set gray stroke color because /'P0' is an invalid float value\n",
      "Cannot set gray stroke color because /'P0' is an invalid float value\n",
      "Cannot set gray stroke color because /'P0' is an invalid float value\n",
      "Cannot set gray stroke color because /'P0' is an invalid float value\n",
      "Cannot set gray stroke color because /'P0' is an invalid float value\n",
      "Cannot set gray stroke color because /'P0' is an invalid float value\n",
      "Cannot set gray stroke color because /'P0' is an invalid float value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents for modeling: 4033\n",
      "   Topic  Count                                          Name  \\\n",
      "0      0   2656  0_research_government_development_technology   \n",
      "1      1    754   1_quantum_computing_technologies_technology   \n",
      "2      2    623              2_quantum_technologies_uk_sector   \n",
      "\n",
      "                                      Representation  \\\n",
      "0  [research, government, development, technology...   \n",
      "1  [quantum, computing, technologies, technology,...   \n",
      "2  [quantum, technologies, uk, sector, support, i...   \n",
      "\n",
      "                                 Representative_Docs  \n",
      "0  [National security needs often drive the advan...  \n",
      "1  [quantum computing technologies. and software,...  \n",
      "2  [commercial success of quantum technologies br...  \n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from bertopic import BERTopic\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# --- PART 1: EXTRACTION (Same as before) ---\n",
    "PDF_DIR = Path(\"countries_edited\")\n",
    "rows = []\n",
    "\n",
    "for pdf_path in PDF_DIR.glob(\"*.pdf\"):\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        full_text = \"\\n\".join([p.extract_text() for p in pdf.pages if p.extract_text()])\n",
    "        \n",
    "        # Increase granularity: split by sentences or small paragraphs\n",
    "        chunks = [c.strip() for c in full_text.split('\\n') if len(c.strip()) > 50]\n",
    "        \n",
    "        stem = pdf_path.stem               \n",
    "        parts = stem.split(\"_\")\n",
    "        country = parts[0] if len(parts) > 0 else \"Unknown\"\n",
    "\n",
    "        for chunk in chunks:\n",
    "            rows.append({\"country\": country, \"text\": chunk, \"file\": pdf_path.name})\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "docs = df['text'].tolist()\n",
    "\n",
    "print(f\"Total documents for modeling: {len(docs)}\")\n",
    "\n",
    "# --- PART 2: THE \"TINY DATA\" CONFIGURATION ---\n",
    "\n",
    "# 1. Use PCA instead of UMAP: PCA is linear and won't crash on small N\n",
    "dim_model = PCA(n_components=min(2, len(docs)-1)) \n",
    "\n",
    "# 2. Use KMeans instead of HDBSCAN: \n",
    "# HDBSCAN requires a minimum density that small data doesn't have.\n",
    "# n_clusters=3 (Adjust this based on how many topics you expect)\n",
    "cluster_model = KMeans(n_clusters=min(3, len(docs)))\n",
    "\n",
    "topic_model = BERTopic(\n",
    "    umap_model=dim_model,\n",
    "    hdbscan_model=cluster_model,\n",
    "    vectorizer_model=CountVectorizer(stop_words=\"english\"),\n",
    ")\n",
    "\n",
    "# --- PART 3: RUN MODELING ---\n",
    "try:\n",
    "    topics, probs = topic_model.fit_transform(docs)\n",
    "    df['topic'] = topics\n",
    "    print(topic_model.get_topic_info())\n",
    "except Exception as e:\n",
    "    print(f\"Error encountered: {e}\")\n",
    "    print(\"If you have fewer than 5 docs, BERTopic might not be the right tool.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "aa1056ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-02 16:32:08,409 - BERTopic - Embedding - Transforming documents to embeddings.\n",
      "Batches: 100%|██████████| 127/127 [00:16<00:00,  7.49it/s]\n",
      "2026-01-02 16:32:27,764 - BERTopic - Embedding - Completed ✓\n",
      "2026-01-02 16:32:27,766 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-02 16:32:58,778 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-02 16:32:58,778 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-01-02 16:32:59,695 - BERTopic - Cluster - Completed ✓\n",
      "2026-01-02 16:32:59,697 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-02 16:32:59,812 - BERTopic - Representation - Completed ✓\n",
      "2026-01-02 16:33:00,029 - BERTopic - WARNING: When you use `pickle` to save/load a BERTopic model,please make sure that the environments in which you saveand load the model are **exactly** the same. The version of BERTopic,its dependencies, and python need to remain the same.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topic  Count                                               Name  \\\n",
      "0     -1   1457                                   -1_and_the_to_of   \n",
      "1      0    253                      0_quantum_technologies_of_the   \n",
      "2      1    123  1_technologies_applications_technology_applica...   \n",
      "3      2    119               2_canada_canadian_government_ontario   \n",
      "4      3    113                           3_uk_quantum_leading_the   \n",
      "\n",
      "                                      Representation  \\\n",
      "0  [and, the, to, of, for, in, quantum, research,...   \n",
      "1  [quantum, technologies, of, the, sector, to, a...   \n",
      "2  [technologies, applications, technology, appli...   \n",
      "3  [canada, canadian, government, ontario, has, e...   \n",
      "4  [uk, quantum, leading, the, business, ensure, ...   \n",
      "\n",
      "                                 Representative_Docs  \n",
      "0  [To continue to grow this offer and ensure qua...  \n",
      "1  [across the different quantum technologies, ou...  \n",
      "2  [applications, as well as an international lev...  \n",
      "3  [work and live in Canada and around the world....  \n",
      "4  [ensure that quantum regulation supports UK bu...  \n",
      "Topic definitions saved to 'topic_definitions.csv'\n"
     ]
    }
   ],
   "source": [
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Prepare the Data\n",
    "# If your PDFs are very long, BERTopic might struggle. \n",
    "# It's often better to use a list of strings (documents).\n",
    "docs = df['text'].tolist()\n",
    "\n",
    "# 2. Configure the sub-models (Optional but recommended for stability)\n",
    "# UMAP reduces dimensions, HDBSCAN clusters them\n",
    "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine', random_state=42)\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=10, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "\n",
    "# 3. Initialize and Fit BERTopic\n",
    "# We use 'all-MiniLM-L6-v2' as it is fast and accurate for general text\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=\"all-MiniLM-L6-v2\",\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    calculate_probabilities=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "topics, probs = topic_model.fit_transform(docs)\n",
    "\n",
    "# 4. Add results back to your original DataFrame\n",
    "df['topic'] = topics\n",
    "\n",
    "# 5. Review the Topics\n",
    "topic_info = topic_model.get_topic_info()\n",
    "print(topic_info.head())\n",
    "\n",
    "# 6. Save the results\n",
    "df.to_csv(\"document_with_topics.csv\", index=False)\n",
    "topic_model.save(\"my_bertopic_model\")\n",
    "\n",
    "\n",
    "# ... (Keep your previous PDF extraction and BERTopic fitting code) ...\n",
    "\n",
    "# 1. Get the topic information table\n",
    "# This contains Topic ID, Count, Name, and Representation (keywords)\n",
    "topic_info = topic_model.get_topic_info()\n",
    "\n",
    "# 2. Save the topic-to-word mapping to a CSV\n",
    "# This creates a file where you can see: Topic 1 -> \"green, energy, hydrogen...\"\n",
    "topic_info.to_csv(\"topic_definitions.csv\", index=False)\n",
    "\n",
    "# 3. (Optional) Get an even more detailed word-score mapping\n",
    "# This creates a \"long\" format CSV with words and their c-TF-IDF scores\n",
    "topic_representation = topic_model.get_topics() # Dictionary format\n",
    "rows_list = []\n",
    "for topic_id, words in topic_representation.items():\n",
    "    for word, score in words:\n",
    "        rows_list.append({\n",
    "            \"topic_id\": topic_id,\n",
    "            \"word\": word,\n",
    "            \"score\": score\n",
    "        })\n",
    "\n",
    "word_score_df = pd.DataFrame(rows_list)\n",
    "word_score_df.to_csv(\"topic_words_with_scores.csv\", index=False)\n",
    "\n",
    "print(\"Topic definitions saved to 'topic_definitions.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d88e5842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Get the topic information table\n",
    "# This contains Topic ID, Count, Name, and Representation (keywords)\n",
    "topic_info = topic_model.get_topic_info()\n",
    "\n",
    "# 2. Save the topic-to-word mapping to a CSV\n",
    "# This creates a file where you can see: Topic 1 -> \"green, energy, hydrogen...\"\n",
    "topic_info.to_csv(\"topic_definitions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9c35005c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_topic_list = [\n",
    "    [\"university\", \"research\", \"science\", \"academia\", \"phd\", \"knowledge\", \"education\"], # Academia\n",
    "    [\"industry\", \"business\", \"innovation\", \"startup\", \"sme\", \"market\", \"commercialization\"], # Industry\n",
    "    [\"government\", \"policy\", \"regulation\", \"ministry\", \"funding\", \"strategy\", \"public\"], # Government\n",
    "    [\"society\", \"citizen\", \"public_engagement\", \"inclusion\", \"civil_society\", \"ngo\", \"participation\"] # Civil Society\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "307290e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-02 16:27:55,739 - BERTopic - Embedding - Transforming documents to embeddings.\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.11it/s]\n",
      "2026-01-02 16:27:59,332 - BERTopic - Embedding - Completed ✓\n",
      "2026-01-02 16:27:59,332 - BERTopic - Guided - Find embeddings highly related to seeded topics.\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 41.59it/s]\n",
      "2026-01-02 16:27:59,366 - BERTopic - Guided - Completed ✓\n",
      "2026-01-02 16:27:59,366 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-02 16:27:59,397 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-02 16:27:59,397 - BERTopic - Cluster - Start clustering the reduced embeddings\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "k must be less than or equal to the number of training points",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     12\u001b[39m topic_model = BERTopic(\n\u001b[32m     13\u001b[39m     seed_topic_list=seed_topic_list,\n\u001b[32m     14\u001b[39m     vectorizer_model=vectorizer_model,\n\u001b[32m     15\u001b[39m     min_topic_size=\u001b[32m10\u001b[39m, \u001b[38;5;66;03m# Adjust based on your dataset size\u001b[39;00m\n\u001b[32m     16\u001b[39m     verbose=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     17\u001b[39m )\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# 4. Fit the model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m topics, probs = \u001b[43mtopic_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# 5. Review the results\u001b[39;00m\n\u001b[32m     23\u001b[39m topic_info = topic_model.get_topic_info()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\OneDrive - TUM\\Master Thesis\\Analysis\\START\\.venv\\Lib\\site-packages\\bertopic\\_bertopic.py:491\u001b[39m, in \u001b[36mBERTopic.fit_transform\u001b[39m\u001b[34m(self, documents, embeddings, images, y)\u001b[39m\n\u001b[32m    487\u001b[39m         umap_embeddings = \u001b[38;5;28mself\u001b[39m.umap_model.transform(embeddings)\n\u001b[32m    489\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(documents) > \u001b[32m0\u001b[39m:\n\u001b[32m    490\u001b[39m     \u001b[38;5;66;03m# Cluster reduced embeddings\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m491\u001b[39m     documents, probabilities = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cluster_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mumap_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    492\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_zeroshot() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(assigned_documents) > \u001b[32m0\u001b[39m:\n\u001b[32m    493\u001b[39m         documents, embeddings = \u001b[38;5;28mself\u001b[39m._combine_zeroshot_topics(\n\u001b[32m    494\u001b[39m             documents, embeddings, assigned_documents, assigned_embeddings\n\u001b[32m    495\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\OneDrive - TUM\\Master Thesis\\Analysis\\START\\.venv\\Lib\\site-packages\\bertopic\\_bertopic.py:3986\u001b[39m, in \u001b[36mBERTopic._cluster_embeddings\u001b[39m\u001b[34m(self, umap_embeddings, documents, partial_fit, y)\u001b[39m\n\u001b[32m   3984\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3985\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3986\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhdbscan_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mumap_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3987\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3988\u001b[39m         \u001b[38;5;28mself\u001b[39m.hdbscan_model.fit(umap_embeddings)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\OneDrive - TUM\\Master Thesis\\Analysis\\START\\.venv\\Lib\\site-packages\\hdbscan\\hdbscan_.py:1291\u001b[39m, in \u001b[36mHDBSCAN.fit\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m   1288\u001b[39m     \u001b[38;5;28mself\u001b[39m.probabilities_ = new_probabilities\n\u001b[32m   1290\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.prediction_data:\n\u001b[32m-> \u001b[39m\u001b[32m1291\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prediction_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1292\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.branch_detection_data:\n\u001b[32m   1293\u001b[39m     \u001b[38;5;28mself\u001b[39m.generate_branch_detection_data()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\OneDrive - TUM\\Master Thesis\\Analysis\\START\\.venv\\Lib\\site-packages\\hdbscan\\hdbscan_.py:1332\u001b[39m, in \u001b[36mHDBSCAN.generate_prediction_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1329\u001b[39m         warn(\u001b[33m\"\u001b[39m\u001b[33mMetric \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m not supported for prediction data!\u001b[39m\u001b[33m\"\u001b[39m.format(\u001b[38;5;28mself\u001b[39m.metric))\n\u001b[32m   1330\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1332\u001b[39m     \u001b[38;5;28mself\u001b[39m._prediction_data = \u001b[43mPredictionData\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1333\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raw_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1334\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcondensed_tree_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1335\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmin_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1336\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtree_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtree_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1337\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1338\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_metric_kwargs\u001b[49m\n\u001b[32m   1339\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1340\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1341\u001b[39m     warn(\n\u001b[32m   1342\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot generate prediction data for non-vector \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1343\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mspace inputs -- access to the source data rather \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1344\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mthan mere distances is required!\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1345\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\OneDrive - TUM\\Master Thesis\\Analysis\\START\\.venv\\Lib\\site-packages\\hdbscan\\prediction.py:103\u001b[39m, in \u001b[36mPredictionData.__init__\u001b[39m\u001b[34m(self, data, condensed_tree, min_samples, tree_type, metric, **kwargs)\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28mself\u001b[39m.raw_data = data.astype(np.float64)\n\u001b[32m    101\u001b[39m \u001b[38;5;28mself\u001b[39m.tree = \u001b[38;5;28mself\u001b[39m._tree_type_map[tree_type](\u001b[38;5;28mself\u001b[39m.raw_data,\n\u001b[32m    102\u001b[39m                                            metric=metric, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28mself\u001b[39m.core_distances = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtree\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmin_samples\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m][:, -\u001b[32m1\u001b[39m]\n\u001b[32m    104\u001b[39m \u001b[38;5;28mself\u001b[39m.dist_metric = DistanceMetric.get_metric(metric, **kwargs)\n\u001b[32m    106\u001b[39m selected_clusters = \u001b[38;5;28msorted\u001b[39m(condensed_tree._select_clusters())\n",
      "\u001b[36mFile \u001b[39m\u001b[32msklearn/neighbors/_binary_tree.pxi:1179\u001b[39m, in \u001b[36msklearn.neighbors._kd_tree.BinaryTree64.query\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: k must be less than or equal to the number of training points"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
