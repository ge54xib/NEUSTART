from bertopic import BERTopic
from sentence_transformers import SentenceTransformer
from sklearn.feature_extraction.text import CountVectorizer
from umap import UMAP
from hdbscan import HDBSCAN
import pandas as pd

docs = [
    "Neural networks are widely used in deep learning applications for natural language processing.", "Convolutional neural networks perform well on image recognition tasks.", "Recurrent neural networks and transformers are popular architectures for sequence modeling.", "Backpropagation is the core algorithm used to train deep neural networks.", "Large language models are trained using massive text corpora and self-supervised learning.", "Support vector machines are effective for high-dimensional classification problems.", "Logistic regression is commonly used for binary classification tasks.", "Linear regression models the relationship between dependent and independent variables.", "Decision trees are interpretable models used in classic machine learning.", "Random forests improve prediction accuracy by aggregating multiple decision trees.", "Latent Dirichlet Allocation is a probabilistic model for discovering topics in text.", "GuidedLDA incorporates seed words to steer topic discovery.", "Topic modeling is commonly used in document clustering and text mining.", "Bag-of-words representations are often used in classical topic models.", "TF-IDF weighting improves term importance estimation in document collections.", "Gradient descent is an optimization algorithm used to minimize loss functions.", "Stochastic gradient descent scales well to large datasets.", "Regularization techniques help prevent overfitting in machine learning models.", "Hyperparameter tuning improves model performance and generalization.", "Cross-validation is used to evaluate machine learning models reliably.", "Machine learning models are applied in computer vision, NLP, and recommendation systems.", "Feature engineering plays a critical role in classic machine learning pipelines.", "Embeddings capture semantic meaning in vector space representations.", "Dimensionality reduction techniques help visualize high-dimensional data.", "Clustering algorithms group similar data points without labels."
]  # deine 25 docs

seed_words = [
    ["neural", "network", "deep", "convolutional"],
    ["regression", "logistic", "svm", "vector", "machine"]
]

embed_model = SentenceTransformer("all-MiniLM-L6-v2")

umap_model = UMAP(
    n_neighbors=min(10, len(docs)-1),  # wichtig gegen k>=N
    n_components=5,
    metric="cosine",
    random_state=42
)

hdbscan_model = HDBSCAN(min_cluster_size=2, min_samples=1)

vectorizer_model = CountVectorizer(
    stop_words="english",
    ngram_range=(1, 3),   # ðŸ‘ˆ erlaubt 1-, 2- und 3-Wort-Phrasen
    min_df=2
)

topic_model = BERTopic(
    embedding_model=embed_model,
   # seed_topic_list=seed_words,       # âœ… seeds richtig
    umap_model=umap_model,            # âœ… UMAP stabil
    hdbscan_model=hdbscan_model,
    vectorizer_model=vectorizer_model, # âœ… stopwords raus
    nr_topics=None,                   # âœ… kein "auto"
    top_n_words=10
)

topics, probs = topic_model.fit_transform(docs)

print(topic_model.get_topic_info())
for topic_id in sorted(set(topics)):
    print(topic_id, topic_model.get_topic(topic_id))



topic_model.visualize_topics()